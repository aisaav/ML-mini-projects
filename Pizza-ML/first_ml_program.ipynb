{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our First ML Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load pizzas and reservations from `pizza.txt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X, Y = np.loadtxt(\"pizza.txt\", skiprows=1, unpack=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the reservations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.,  2., 14., 23., 13., 13.,  1., 18.,  7., 10., 26.,  3.,  3.,\n",
       "       21., 22.,  2., 27.,  6., 10., 18., 15.,  9., 26.,  8., 15., 10.,\n",
       "       21.,  5.,  6., 13.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and these are the pizzas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([33., 16., 32., 51., 27., 25., 16., 34., 22., 17., 29., 15., 15.,\n",
       "       32., 37., 13., 44., 16., 21., 37., 30., 26., 34., 23., 39., 27.,\n",
       "       37., 17., 18., 23.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot pizzas and reservations with Matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAETCAYAAAAh/OHhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAdLklEQVR4nO3dfbRcVZnn8e8vNygEkDcDRCAJKApoIz3cRhi0BeRNpA3TI4zO1Yk23VkjzYhvg2BsBdvQ2r6AuppubzdIXASEURFUpm1etREbDYIKAgYhyQCBBEkADaJJnvlj7+JWiqqbe+rWrXOq6vdZq9aps+ucU0+d5NZTZ++z91ZEYGZmNlHTyg7AzMx6ixOHmZkV4sRhZmaFOHGYmVkhThxmZlaIE4eZmRUyvewAJC0HngY2AhsiYljSzsAVwFxgOXBKRKwtK0YzMxtTlSuOIyPioIgYzutnATdExL7ADXndzMwqoCqJo9E8YHF+vhg4qcRYzMysjsruOS7pQWAtEMCXImJU0rqI2LFum7URsVOTfRcACwC23Xbbg/fbb79uhW1m1hduv/32xyNiZpF9Sm/jAA6PiEck7QpcJ+neie4YEaPAKMDw8HAsXbp0qmI0M+tLklYU3af0qqqIeCQvVwNXAYcAj0maBZCXq8uL0MzM6pWaOCRtK2n72nPgWOAu4Bpgft5sPnB1ORGamVmjsquqdgOuklSL5bKI+FdJPwaulHQqsBI4ucQYzcysTqmJIyIeAF7dpPzXwBu6H5GZmW1J6W0cZmbWW5w4zMysECcOMzMrxInDzMwKceIwM7NCnDjMzKwQJw4zMyvEicPMzApx4jAzs0KcOMzMrBAnDjMzK8SJw8zMCnHiMDOzQpw4zMysECcOMzMrxInDzMwKceIwM7NCnDjMKmbJEpg7F6ZNS8slS8qOyGxzZc85bmZ1liyBBQtg/fq0vmJFWgcYGSkvLrN6vuIwq5CFC8eSRs369ancrCqcOMwqZOXKYuVmZXDiMKuQ2bOLlZuVwYnDrEIWLYIZMzYvmzEjlZtVhROHWYWMjMDoKMyZA1Jajo66YdyqxXdVmVXMyIgThVWbrzjMzKwQJw4zMyvEicPMzApx4jAzs0KcOMzMrBAnDjMzK8SJw8zMCnHiMDOzQpw4zMysECcOMzMrpBKJQ9KQpDskfTuv7y3pNknLJF0h6QVlx2hmZkklEgdwBnBP3fqngPMjYl9gLXBqKVGZmdnzlJ44JO0JvAn4l7wu4Cjga3mTxcBJ5URnZmaNSk8cwAXAmcCmvL4LsC4iNuT1h4A9mu0oaYGkpZKWrlmzZuojNTOzchOHpBOB1RFxe31xk02j2f4RMRoRwxExPHPmzCmJ0YpZsgTmzoVp09JyyZKyIzKzTit7Po7DgTdLOgHYGngR6QpkR0nT81XHnsAjJcZoE7RkCSxYAOvXp/UVK9I6eH4Js35S6hVHRJwdEXtGxFzgrcCNETEC3AS8JW82H7i6pBCtgIULx5JGzfr1qdzM+kcV2jia+RDwfkn3k9o8Lio5HpuAlSuLlZtZbyq7quo5EXEzcHN+/gBwSJnxWHGzZ6fqqWblZtY/qnrFYT1o0SKYMWPzshkzUrmZ9Q8nDuuYkREYHYU5c0BKy9FRN4yb9ZvKVFVZfxgZcaIw63e+4jAzs0KcOKzSOt2hcLzjufOi2cS4qsoqq9MdCsc7HrjzotlEKaLpaB49Z3h4OJYuXVp2GNZBc+c2v713zhxYvryzx4POvpdZr5B0e0QMF9nHVxxWWZ3uUNjO8dx50ez53MZhldWq42C7HQrHO16n38usnzlxWGV1ukPheMdz50WziXPisMrqdIfC8Y7nzotmE+fGcTOzAdZO47ivOMzMrBAnDjMzK8SJw8zMCnHiMDOzQpw4zMysECcOq7R+HXiwXz+XDQYPOWKV1elBDquiXz+XDQ7347DK6vQgh1XRr5/LepP7cVhf6fQgh1XRr5/LBocTh7VUdj18vw482K+fywaHE4c1VauHX7ECIsbq4buZPPp14MF+/Vw2OJw4rKmFC8cab2vWr0/l3dKvAw/26+eyweHGcWtq2rR0pdFIgk2buh+PmU0NN45bx7ge3sxaceKwprZUD192w3m7ejVusypxB0BrqlbfvnBhuk109uyUNEZGercDW6/GbVY1buOwwnq1A1uvxm02ldzGYV3Rqx3YejVus6px4rDCerXhvFfjNqsaJw4rrFc7sPVq3GZV48RhhfVqB7ZejdusajrWOC5pK+BVwPqIuK8jBy3AjeNmZsV1pXFc0imSrpS0c13ZS4G7gaXALyR9Q9IWb/WVtLWkH0n6qaS7JZ2by/eWdJukZZKukPSConFaOdxPwqz/tVNV9RfAfhHxRF3ZZ4GXATcBPwPmAe+awLGeBY6KiFcDBwHHSzoU+BRwfkTsC6wFTm0jTuuyKgyMaGZTr53EcQDw49qKpBcBJwBXRsTRwCHAvUwgcUTym7y6VX4EcBTwtVy+GDipjTity6owMKKZTb12EsdMYFXd+mGkHuhfBYiIPwDXAS+dyMEkDUm6E1id9/sVsC4iNuRNHgL2aLHvAklLJS1ds2ZNGx/FOsn9JMwGQzuJ42lgh7r115OuEm6pK/sdsP1EDhYRGyPiIGBP0tXK/s02a7HvaEQMR8TwzJkzJ/J2NoXcT8JsMLSTOJYBb5T0wtxofTLws4h4vG6bOaQriAmLiHXAzcChwI51jet7Ao+0Ead1mftJTJxvIrBe1k7iGAX2ISWQe/Lzixu2eQ3pLqtxSZopacf8fBvg6HzMm4C35M3mA1e3Ead1mftJTIxvIrBe11Y/DknnAXlcUZYA7418IElHAdcDZ0bEZ7ZwnANJjd9DpCR2ZUR8XNI+pDaTnYE7gLdHxLPjHcv9OKxXeLBFq5J2+nF0fHTcXH21DfDbugbuKefEYb3CsytalVRidNyI+H1EPNnNpGHWS3wTgfU6j1Vl1mW+icB6XVuJQ9IsSf8g6X5Jz0ja2OThKw6zJnwTgfW6wlPHStoD+BGwG+nOqRcCK0jDh+yTj3kn8GTnwjTrLyMjThTWu9q54vgosDtwfB5jCuDLEbEfKXF8l9Q4/uedCdEmqtN9A9zXwMyaaSdxHAf8a0Rc3/hCRDxE6hC4DXDuJGOzAjrdN8B9DcyslXYSx+5s3rlvIylRAJAHLbyONEKudUmnBxj0gIVm1ko7ieMpoH5+jLU8fxDCJ0mDIVqXdHqAQQ9YaGattJM4VgB71a3/FDhK0gwASdOAY0mj2lqXdLpvgPsamFkr7SSOG4Aj81SxkIYMeQlwq6RPAz8AXglc0ZkQrV6rButO9w1wXwMza6Xw7bjARaTqqRcDqyLiUkkHA/8LODBv81XAXzEdVmuwrrU91BqsYezWzoULU3XS7NnpS77dWz47fTwz6x8dG6tK0kzS7bjLI+Kxjhy0gEEYq8qD45lZp7UzVlU7VxxNRcQawNPwTSE3WJtZFRRu45B0saQvStp5nG3mSWqco8MmqdsN1u4AaGbNtNM4/k7gNFJj+D4ttjmINAGTdVA3G6zdAdDMWml3dNw7SO0ZP5T0nzsYj42jm4PjuQOgmbXSbuK4BjgB2Bq4XtIpnQvJxjMykhrCN21Ky6m6y8ntKWbWStvzceSxqg4nNYhfJulDHYvKSucOgGbWyqQmcoqIu4DXkHqPnydpVNJQRyKzUrkDoJm1MukZACPiUeB1wHeAvwSuBXaY7HGtXJ5syMxa6Ug/johYL2ke8HngdOANnTiulcuTDZlZM+0OcriusTCS9wDvBzTZwMzMrJoKJ46I2DsivjDO6xeQBj1s1cfDtqAqHe+qEoeZVUs7c47PBn4XEavH2ewZ0lzkVtBEBjIcpDjMrHraqapaDjwk6fRxtnkf8GBbEQ24qnS8q0ocZlY97d5VNQR8XtL5nQzGqtPxripxmFn1tJs4LgBuAs6QdJWkbba0gz1fszaEqnS8q0ocZlY97SaOJ4HjgS8D84CbJe3WsagGQKtBBE84oRod79wB0MxamcyQIxsi4lTgb4Bh0oCH+3cssj7Xqg3h2mur0fHOHQDNrJXCMwBK2gScExEfryt7G3Ax6W6qk4HXAh+NiK4NP9JrMwBOm5auNBpJaQBDM7NuaGcGwEkPOQIQEZcDxwKbSEOOnNiJ4/azqWhDcL8LM+uGjiQOgIj4d+AwYCVwcKeO26863YbgiZfMrFvaSRzvAq5u9kJELAMOBf4J+Mok4up7nW5DcL8LM+uWwm0cVdVrbRyd5jYTM2tHaW0cVj73uzCzbtniWFWSLgYC+HBEPJbXJyLy7brjHXsvUpXW7qSG9dGI+LyknYErgLmkIU5OiYi1E3zfgbRo0eZjS4H7XZjZ1NhiVVW+/TaA/SPil3l9ImJLt+NKmgXMioifSNoeuB04CXgn8EREfFLSWcBOETHu1LSDXlUFqSF84cI0LMjs2SlpuN+FmY2nnaqqiYyOu3dePtywPmkRsQpYlZ8/LekeYA9Sb/Qj8maLgZsBz2m+BZ54ycy6YYuJIyJWjLfeKZLmAn8M3AbslpMKEbFK0q4t9lkALACY7cp8M7OuKNQ4Lmm2pP8q6c9z+0RHSNoO+Drw3oh4aqL7RcRoRAxHxPDMmTM7FY6ZmY1jwolD0meAB4Argf8DPCjp05MNQNJWpKSxJCK+kYsfy+0ftXaQ8SaNMjOzLppQ4pD03xmbS/xe4L78/P15nKq2SBJwEXBPRHyu7qVrgPn5+XxadDg0M7Pum+gVx6nABuDoiHhlRBwAHEe6hXbcW2634HDgHcBRku7MjxOATwLHSFoGHJPXzcysAiY65/iBwDcj4qZaQURcL+lqxu5+KiwibiFduTTzhnaPa2ZmU2eiVxw7kaqnGt0L7Ni5cPqPR6w1s34z0SuOacAfmpT/gdZXDAOvNmJtrTd3bcRacH8LM+tdRW7H7Y/RELvII9aaWT+a6BUHwDmSzmn2gqSNTYojIoocv++sXFms3MysFxS54lDBx8CMvNuqHaPbI9a6PcXMumFCVwQRMTBJoKjx2jG6OWKt21PMrFs8kdMkzZ2bvqQbzZkDy5d3b8TaLcVhZtZMO6PjOnFMUlVm3qtKHGbWWzwDYINWdf6dbAvYUjtGt9odPAOgmXVNRPTF4+CDD456l14aMWNGRPodnh4zZkS8+93Nyy+9NNrS6n0uvXT81zqtm+9lZv0DWBoFv2/7tqqqVZ3/0BBsbHLz8GTaAlq1Y3S73cEzAJpZUW7jqEscrer8W5mKtgC3O5hZ1bmNo06ruv2hFrOgT0VbgNsdzKwf9W3iWLQo9ZmoN2NG6tvQrLzWt+K002D69HRVMH16Wu90DFPRj8PMrFv6NnGMjMDoaGpPkNJydBQuvLB5+chIShL/+I9jbSAbN6b1dpNHqxjc7mBmvaxv2zjaMX1684bzoSHYsGFShzYzqyS3cUxSs6QxXrmZ2SBy4qjTquG8VbmZ2SBy4qhTGxRwouVmZoNooOfLaHThhWk5Opqqp4aGUtKolZuZmRPH81x4oROFmdl4BrKqyhMemZm1b+CuODzhkZnZ5AzcFcfChZvPyAdpfeHCcuIxM+s1A5c4Vq4sVm5mZpsbuMThgQfNzCZn4BKHBx40M5ucgUscHnjQzGxyBu6uKkhJwonCzKw9A3fFYWZmk+PEYWZmhThxmJlZIU4cZmZWiBOHmZkVUmrikHSxpNWS7qor21nSdZKW5eVOZcZoZmabK/uK4xLg+Iays4AbImJf4Ia8bmZmFVFq4oiI7wNPNBTPAxbn54uBk7oalJmZjavsK45mdouIVQB5uWurDSUtkLRU0tI1a9Z0LUAzs0FWxcQxYRExGhHDETE8c+bMssMxMxsIVUwcj0maBZCXq0uOx8zM6lQxcVwDzM/P5wNXlxiLmZk1KPt23MuBHwKvkPSQpFOBTwLHSFoGHJPXzcysIkodHTci3tbipTd0NRAzM5uwKlZVmZlZhTlxmJlZIU4cZmZWiBOHmZkV4sRhZmaFOHGYmVkhThxmZlaIE4eZmRXixGFmZoU4cZiZWSFOHGZmVogTh5mZFeLEYWZmhThxmJlZIU4cZmZWiBOHmZkV4sRhZmaFOHGYmVkhThxmZlaIE4eZmRXixGFmZoU4cZiZWSFOHGZmVogTh5mZFeLEYWZmhThxmJlZIU4cZmZWiBOHmZkV4sRhZmaFOHGYmVkhThxmZlaIE4eZmRXixGFmZoU4cZiZWSFOHGZmVkhlE4ek4yXdJ+l+SWeVHY+ZmSWVTByShoB/AN4IHAC8TdIB5UZlZmZQ0cQBHALcHxEPRMTvga8C80qOyczMgOllB9DCHsD/q1t/CHhN40aSFgAL8uqzku7qQmy94MXA42UHURE+F2N8Lsb4XIx5RdEdqpo41KQsnlcQMQqMAkhaGhHDUx1YL/C5GONzMcbnYozPxRhJS4vuU9WqqoeAverW9wQeKSkWMzOrU9XE8WNgX0l7S3oB8FbgmpJjMjMzKlpVFREbJJ0OfBcYAi6OiLu3sNvo1EfWM3wuxvhcjPG5GONzMabwuVDE85oOzMzMWqpqVZWZmVWUE4eZmRXS84lj0IcmkXSxpNX1fVgk7SzpOknL8nKnMmPsBkl7SbpJ0j2S7pZ0Ri4fxHOxtaQfSfppPhfn5vK9Jd2Wz8UV+caTgSBpSNIdkr6d1wfyXEhaLunnku6s3Ybbzt9ITycOD00CwCXA8Q1lZwE3RMS+wA15vd9tAD4QEfsDhwJ/nf8vDOK5eBY4KiJeDRwEHC/pUOBTwPn5XKwFTi0xxm47A7inbn2Qz8WREXFQXT+Wwn8jPZ048NAkRMT3gScaiucBi/PzxcBJXQ2qBBGxKiJ+kp8/TfqS2IPBPBcREb/Jq1vlRwBHAV/L5QNxLgAk7Qm8CfiXvC4G9Fy0UPhvpNcTR7OhSfYoKZYq2S0iVkH6QgV2LTmerpI0F/hj4DYG9Fzkqpk7gdXAdcCvgHURsSFvMkh/KxcAZwKb8vouDO65CODfJN2eh2yCNv5GKtmPo4AJDU1ig0PSdsDXgfdGxFPpx+XgiYiNwEGSdgSuAvZvtll3o+o+SScCqyPidklH1IqbbNr35yI7PCIekbQrcJ2ke9s5SK9fcXhokuYekzQLIC9XlxxPV0jaipQ0lkTEN3LxQJ6LmohYB9xMavfZUVLtx+Kg/K0cDrxZ0nJSVfZRpCuQQTwXRMQjebma9IPiENr4G+n1xOGhSZq7Bpifn88Hri4xlq7I9dYXAfdExOfqXhrEczEzX2kgaRvgaFKbz03AW/JmA3EuIuLsiNgzIuaSvh9ujIgRBvBcSNpW0va158CxwF208TfS8z3HJZ1A+gVRG5pkUckhdZWky4EjSMNEPwZ8DPgmcCUwG1gJnBwRjQ3ofUXSa4F/B37OWF32h0ntHIN2Lg4kNXIOkX4cXhkRH5e0D+lX987AHcDbI+LZ8iLtrlxV9cGIOHEQz0X+zFfl1enAZRGxSNIuFPwb6fnEYWZm3dXrVVVmZtZlThxmZlaIE4eZmRXixGFmZoU4cZiZWSFOHGZ9QtIlkiIPuWI2ZZw4rCvyF1r9Y6OkJyTdLOmdGtSxQQqQdE4+d0eUHYsNtl4fq8p6z7l5uRXwMuC/AK8HhoHTywqqT5wNfBJ4uOxArL85cVhXRcQ59euSDge+D5wm6bMR8WApgfWBPLLpqrLjsP7nqiorVUT8ALiXNGLpwc22kXScpGslPS7pWUm/kvTp2nhMDdseKOnyPNPZs5LWSPqJpAvyIIj1206XdJqk/5D0lKT1eZa40yVNa9h2bq4mukTSy/OscaslbZJ0hKR7Jf1e0otbfIaz8v5/XVd2pKRRSb/I7/+MpLskfUzS1g37LycNJwNwU321X902Lds4JJ0i6fuSnszv83NJZ0t6YZNtl+fHjHyeV+Zzeb+kDzWrVpT0Zkk3SFqVt31E0vckndbsfFhv8xWHVUHti+gPz3tB+iipeusJ4NukkTsPBD4InCDpsIh4Km97IGlsqiAN3PYg8CJSldhpwEdq75GTyLeA44D7gMuA3wFHAl8EXgO8o0msL83v8UtgCbAN8BRpbKjzgLfl/Rv9D6A22VjNh4D9gFuB7wBbk0ZzPQc4QtLReXh0SOOxnUSq1lsMLG/yHk1JOo9UjfV4/py/Ic2aeR5wnKRjIqLx3G8F/BvwEuD/kmZYPIlUFbY1Y1WOKM3r8CXgUdI5fZw0p8OBwLuACycaq/WIiPDDjyl/kL7Mo0n5nwIbSdOdzmp47ci8363Ajg2vvTO/dn5d2Wdz2bwm77MTMK1u/Zy87ReBobryIdIou5sdB5hb+wzAeU2Ov0f+HEubvPYneb+vN5TvQx4vrqH8b/P2/62hvBbzES3O8SX59bl1ZYflspXA7nXl00lf8gF8uOE4y3P5tcA2deW7AuvyY6u68tvzv9+uTWJ6cdn/9/zo/MNVVdZV+c6gcyQtknQFcD3piuODkWchq/OevPyrSPNKPCciLgHuBEaavM0zjQURsTYiNuUYppEa4h8F3hdjv+rJzz9A+uJsduzHqPu1Xbffw6T5mg+W9MqGl2tDVi9u2OeByN+uDS7Iy+OavFbUX+TlJyLi0br33kD6nJuAv2yx73si4pm6fVaThtzeAXhFw7YbaHLFGBGPtx+6VZWrqqzbPtawHsCpEfHlJtseRvoyOlnSyU1efwEwU9IuEfFr4ArgDOCbkr5GSko/iIhfNez3ctL0ocuAj7S4E/gZms+a99NoPfz2JcAxpERxJoDG5olZQ/oF/5w8J8IZpDvLXg5sz+az03ViOtP/lJc3Nr4QEb+U9BCwt6QdG5LzkxFxf5Pj1aZq3qmubAnpau/u/GPge6Tzvmby4VsVOXFYV0WE4LkvzcNI1UL/JGlFRDR+ue1C+j/amGwabQf8OiJ+JOl1wELSJD3vyO91H3BuRFxed1yAfbdw7O2alD3apKzmKlJ7x9slnZ2vXk7M73dBjM1xXWtjuZE0A9tdpKS3hrFf7R8Dntdw3YYd8rLV3VarSPMw7ECqgqpZ13xzap9hqFYQEZ+T9DipHek9wHuBkPQ94H9HxNI2Y7eKclWVlSIifhsR1wN/RvoSWixpRsNmTwJrI0JbeKyoO+4PI+JE0i/iw0ntBbsBl0k6uu64AFdt4bh7Nwt9nM/0DGlCnFmkKw9oUU0FzCMljcUR8UcRsSAiFka6XflLrd6jDbXPunuL12c1bNeWiPhKRBxKSpJvIv0g+FPgu0rzW1sfceKwUkXEz4B/Js37/L6Gl/8D2KlJm8FEjvtsRNwaER9lrK1kXl7eS/pFfWjjLbodcElezs+35r4R+FlE3Nmw3cvy8utNjvH6FseutcUMtXi9mTvy8ojGFyS9jHTeH2xsQ2pXRKyLiGsj4q9I52Jn4HWdOLZVhxOHVcEnSLfCflBSfd35+Xn5z5Je0riT0hzKh9atv07SDo3bka44ANbDcw3DXyT92v6C0rzcjceeJemAoh8kUr+UZaQk9W7Sba2XNNl0eV4e0fC++wCfanH4X+fl7AIhXZyXH5E0s+59hoDPkL4DLipwvOeRdLykZtXetSuN9ZM5vlWP2zisdBHxsKQvkRqKzyT1OSAibpB0FvB3wDJJ15L6ZmwHzCH9Mr8FOD4f6gPAsZJuBh4g9Vd4JelX/1pgtO5t/xZ4NfA/gT+TdCNpqI5dSW0fh5PaSn7Rxkf6Sj7+35DaBC5rss23gPuB90v6I9KVwWxSm8h3aJ4cbiLdBfV3kl6VPxMR8YlWgUTErZL+nnRe78o3DfyWdE5eRTp/n27jM9b7KvA7SbeQEqJIVxl/QrpV9/pJHt+qpuz7gf0YjAct+nHUvb4b6Qvtt8BuDa+9ltR28AipE90a0q24nwOG67Y7Fvgy6cv+yXys+4AvAHOavKdIDeg3kDoY/p6UPG4BPgzsVbft3PwZLpnAZ51NqlYK4FvjbLcX6Y6kh0l3cd1N+oKfnve9uck+b8+f/ZnGc0qTfhx1r701f66nSVd3d5MS49ZNtl0OLG8R8zk09CUhJd+rSMl6fT6Xd+TPsn3Z//f86PxD+R/ezMxsQtzGYWZmhThxmJlZIU4cZmZWiBOHmZkV4sRhZmaFOHGYmVkhThxmZlaIE4eZmRXixGFmZoX8f/Qo1SaVAIWFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This directive tells Jupyter to draw matplotlib plots inside the web page:\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel(\"Reservations\", fontsize=20)  # Print the X label\n",
    "plt.ylabel(\"Pizzas\", fontsize=20)        # Print the Y label\n",
    "plt.axis([0, 50, 0, 50])                 # Both axes range from 0 to 50\n",
    "plt.plot(X, Y, \"bo\")                     # Plot the data as blue circles (that's what \"bo\" stands for)\n",
    "plt.show()                               # Visualize the diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're about to approximate these data with linear regression–that is, with a line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation of a line is `y = x * w + b`. Translate it to code, and you get the `predict()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b):\n",
    "    return X * w + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we already found a line that approximates the points, and this line has `w = 2.1` and `b = 7.3`. How many pizzas should we expect to sell if we got 10 reservations? Here's the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(14, 1.2, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can pass an entire NumPy array to `predict()` instead of a single number. NumPy automatically applies the multiplication and the sum inside `predict()` to all the elements in the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28.8, 18. , 20.4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([14, 5, 7])\n",
    "predict(X, 1.2, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we implement training, we have to define a loss function–a measure of how wrong a line is at approximating the dataset. We'll use the “mean squared error” formula for the loss. This function takes the dataset (`X` and `Y`) and a line (`w` and `b`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, Y, w, b):\n",
    "    predictions = predict(X, w, b)\n",
    "    return np.average((predictions - Y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out with the data from `pizza.txt` and two made-up values for the line's parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = np.loadtxt(\"pizza.txt\", skiprows=1, unpack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.778666666666677"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(X, Y, 1.2, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here's the all-important `train()` function. It takes a dataset, and it returns a line that approximates it. It also takes a number of iterations and a learning rate (`lr`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, iterations, lr):\n",
    "    w = b = 0\n",
    "    for i in range(iterations):\n",
    "        current_loss = loss(X, Y, w, b)\n",
    "        print(\"Iteration %4d => Loss: %.6f\" % (i, current_loss))\n",
    "\n",
    "        if loss(X, Y, w - lr, b) < current_loss:\n",
    "            w -= lr\n",
    "        elif loss(X, Y, w + lr, b) < current_loss:\n",
    "            w += lr\n",
    "        elif loss(X, Y, w, b - lr) < current_loss:\n",
    "            b -= lr\n",
    "        elif loss(X, Y, w, b + lr) < current_loss:\n",
    "            b += lr\n",
    "        else:\n",
    "            return w, b\n",
    "\n",
    "    raise Exception(\"Couldn't find a result within %d iterations\" % iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have everything we need to find a line that approximates our `pizza.txt` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 => Loss: 812.866667\n",
      "Iteration    1 => Loss: 804.820547\n",
      "Iteration    2 => Loss: 796.818187\n",
      "Iteration    3 => Loss: 788.859587\n",
      "Iteration    4 => Loss: 780.944747\n",
      "Iteration    5 => Loss: 773.073667\n",
      "Iteration    6 => Loss: 765.246347\n",
      "Iteration    7 => Loss: 757.462787\n",
      "Iteration    8 => Loss: 749.722987\n",
      "Iteration    9 => Loss: 742.026947\n",
      "Iteration   10 => Loss: 734.374667\n",
      "Iteration   11 => Loss: 726.766147\n",
      "Iteration   12 => Loss: 719.201387\n",
      "Iteration   13 => Loss: 711.680387\n",
      "Iteration   14 => Loss: 704.203147\n",
      "Iteration   15 => Loss: 696.769667\n",
      "Iteration   16 => Loss: 689.379947\n",
      "Iteration   17 => Loss: 682.033987\n",
      "Iteration   18 => Loss: 674.731787\n",
      "Iteration   19 => Loss: 667.473347\n",
      "Iteration   20 => Loss: 660.258667\n",
      "Iteration   21 => Loss: 653.087747\n",
      "Iteration   22 => Loss: 645.960587\n",
      "Iteration   23 => Loss: 638.877187\n",
      "Iteration   24 => Loss: 631.837547\n",
      "Iteration   25 => Loss: 624.841667\n",
      "Iteration   26 => Loss: 617.889547\n",
      "Iteration   27 => Loss: 610.981187\n",
      "Iteration   28 => Loss: 604.116587\n",
      "Iteration   29 => Loss: 597.295747\n",
      "Iteration   30 => Loss: 590.518667\n",
      "Iteration   31 => Loss: 583.785347\n",
      "Iteration   32 => Loss: 577.095787\n",
      "Iteration   33 => Loss: 570.449987\n",
      "Iteration   34 => Loss: 563.847947\n",
      "Iteration   35 => Loss: 557.289667\n",
      "Iteration   36 => Loss: 550.775147\n",
      "Iteration   37 => Loss: 544.304387\n",
      "Iteration   38 => Loss: 537.877387\n",
      "Iteration   39 => Loss: 531.494147\n",
      "Iteration   40 => Loss: 525.154667\n",
      "Iteration   41 => Loss: 518.858947\n",
      "Iteration   42 => Loss: 512.606987\n",
      "Iteration   43 => Loss: 506.398787\n",
      "Iteration   44 => Loss: 500.234347\n",
      "Iteration   45 => Loss: 494.113667\n",
      "Iteration   46 => Loss: 488.036747\n",
      "Iteration   47 => Loss: 482.003587\n",
      "Iteration   48 => Loss: 476.014187\n",
      "Iteration   49 => Loss: 470.068547\n",
      "Iteration   50 => Loss: 464.166667\n",
      "Iteration   51 => Loss: 458.308547\n",
      "Iteration   52 => Loss: 452.494187\n",
      "Iteration   53 => Loss: 446.723587\n",
      "Iteration   54 => Loss: 440.996747\n",
      "Iteration   55 => Loss: 435.313667\n",
      "Iteration   56 => Loss: 429.674347\n",
      "Iteration   57 => Loss: 424.078787\n",
      "Iteration   58 => Loss: 418.526987\n",
      "Iteration   59 => Loss: 413.018947\n",
      "Iteration   60 => Loss: 407.554667\n",
      "Iteration   61 => Loss: 402.134147\n",
      "Iteration   62 => Loss: 396.757387\n",
      "Iteration   63 => Loss: 391.424387\n",
      "Iteration   64 => Loss: 386.135147\n",
      "Iteration   65 => Loss: 380.889667\n",
      "Iteration   66 => Loss: 375.687947\n",
      "Iteration   67 => Loss: 370.529987\n",
      "Iteration   68 => Loss: 365.415787\n",
      "Iteration   69 => Loss: 360.345347\n",
      "Iteration   70 => Loss: 355.318667\n",
      "Iteration   71 => Loss: 350.335747\n",
      "Iteration   72 => Loss: 345.396587\n",
      "Iteration   73 => Loss: 340.501187\n",
      "Iteration   74 => Loss: 335.649547\n",
      "Iteration   75 => Loss: 330.841667\n",
      "Iteration   76 => Loss: 326.077547\n",
      "Iteration   77 => Loss: 321.357187\n",
      "Iteration   78 => Loss: 316.680587\n",
      "Iteration   79 => Loss: 312.047747\n",
      "Iteration   80 => Loss: 307.458667\n",
      "Iteration   81 => Loss: 302.913347\n",
      "Iteration   82 => Loss: 298.411787\n",
      "Iteration   83 => Loss: 293.953987\n",
      "Iteration   84 => Loss: 289.539947\n",
      "Iteration   85 => Loss: 285.169667\n",
      "Iteration   86 => Loss: 280.843147\n",
      "Iteration   87 => Loss: 276.560387\n",
      "Iteration   88 => Loss: 272.321387\n",
      "Iteration   89 => Loss: 268.126147\n",
      "Iteration   90 => Loss: 263.974667\n",
      "Iteration   91 => Loss: 259.866947\n",
      "Iteration   92 => Loss: 255.802987\n",
      "Iteration   93 => Loss: 251.782787\n",
      "Iteration   94 => Loss: 247.806347\n",
      "Iteration   95 => Loss: 243.873667\n",
      "Iteration   96 => Loss: 239.984747\n",
      "Iteration   97 => Loss: 236.139587\n",
      "Iteration   98 => Loss: 232.338187\n",
      "Iteration   99 => Loss: 228.580547\n",
      "Iteration  100 => Loss: 224.866667\n",
      "Iteration  101 => Loss: 221.196547\n",
      "Iteration  102 => Loss: 217.570187\n",
      "Iteration  103 => Loss: 213.987587\n",
      "Iteration  104 => Loss: 210.448747\n",
      "Iteration  105 => Loss: 206.953667\n",
      "Iteration  106 => Loss: 203.502347\n",
      "Iteration  107 => Loss: 200.094787\n",
      "Iteration  108 => Loss: 196.730987\n",
      "Iteration  109 => Loss: 193.410947\n",
      "Iteration  110 => Loss: 190.134667\n",
      "Iteration  111 => Loss: 186.902147\n",
      "Iteration  112 => Loss: 183.713387\n",
      "Iteration  113 => Loss: 180.568387\n",
      "Iteration  114 => Loss: 177.467147\n",
      "Iteration  115 => Loss: 174.409667\n",
      "Iteration  116 => Loss: 171.395947\n",
      "Iteration  117 => Loss: 168.425987\n",
      "Iteration  118 => Loss: 165.499787\n",
      "Iteration  119 => Loss: 162.617347\n",
      "Iteration  120 => Loss: 159.778667\n",
      "Iteration  121 => Loss: 156.983747\n",
      "Iteration  122 => Loss: 154.232587\n",
      "Iteration  123 => Loss: 151.525187\n",
      "Iteration  124 => Loss: 148.861547\n",
      "Iteration  125 => Loss: 146.241667\n",
      "Iteration  126 => Loss: 143.665547\n",
      "Iteration  127 => Loss: 141.133187\n",
      "Iteration  128 => Loss: 138.644587\n",
      "Iteration  129 => Loss: 136.199747\n",
      "Iteration  130 => Loss: 133.798667\n",
      "Iteration  131 => Loss: 131.441347\n",
      "Iteration  132 => Loss: 129.127787\n",
      "Iteration  133 => Loss: 126.857987\n",
      "Iteration  134 => Loss: 124.631947\n",
      "Iteration  135 => Loss: 122.449667\n",
      "Iteration  136 => Loss: 120.311147\n",
      "Iteration  137 => Loss: 118.216387\n",
      "Iteration  138 => Loss: 116.165387\n",
      "Iteration  139 => Loss: 114.158147\n",
      "Iteration  140 => Loss: 112.194667\n",
      "Iteration  141 => Loss: 110.274947\n",
      "Iteration  142 => Loss: 108.398987\n",
      "Iteration  143 => Loss: 106.566787\n",
      "Iteration  144 => Loss: 104.778347\n",
      "Iteration  145 => Loss: 103.033667\n",
      "Iteration  146 => Loss: 101.332747\n",
      "Iteration  147 => Loss: 99.675587\n",
      "Iteration  148 => Loss: 98.062187\n",
      "Iteration  149 => Loss: 96.492547\n",
      "Iteration  150 => Loss: 94.966667\n",
      "Iteration  151 => Loss: 93.484547\n",
      "Iteration  152 => Loss: 92.046187\n",
      "Iteration  153 => Loss: 90.651587\n",
      "Iteration  154 => Loss: 89.300747\n",
      "Iteration  155 => Loss: 87.993667\n",
      "Iteration  156 => Loss: 86.730347\n",
      "Iteration  157 => Loss: 85.510787\n",
      "Iteration  158 => Loss: 84.334987\n",
      "Iteration  159 => Loss: 83.202947\n",
      "Iteration  160 => Loss: 82.114667\n",
      "Iteration  161 => Loss: 81.070147\n",
      "Iteration  162 => Loss: 80.069387\n",
      "Iteration  163 => Loss: 79.112387\n",
      "Iteration  164 => Loss: 78.199147\n",
      "Iteration  165 => Loss: 77.329667\n",
      "Iteration  166 => Loss: 76.503947\n",
      "Iteration  167 => Loss: 75.721987\n",
      "Iteration  168 => Loss: 74.983787\n",
      "Iteration  169 => Loss: 74.289347\n",
      "Iteration  170 => Loss: 73.638667\n",
      "Iteration  171 => Loss: 73.031747\n",
      "Iteration  172 => Loss: 72.468587\n",
      "Iteration  173 => Loss: 71.949187\n",
      "Iteration  174 => Loss: 71.473547\n",
      "Iteration  175 => Loss: 71.041667\n",
      "Iteration  176 => Loss: 70.653547\n",
      "Iteration  177 => Loss: 70.309187\n",
      "Iteration  178 => Loss: 70.008587\n",
      "Iteration  179 => Loss: 69.751747\n",
      "Iteration  180 => Loss: 69.538667\n",
      "Iteration  181 => Loss: 69.369347\n",
      "Iteration  182 => Loss: 69.243787\n",
      "Iteration  183 => Loss: 69.161987\n",
      "Iteration  184 => Loss: 69.123947\n",
      "Iteration  185 => Loss: 69.052847\n",
      "Iteration  186 => Loss: 68.981947\n",
      "Iteration  187 => Loss: 68.911247\n",
      "Iteration  188 => Loss: 68.840747\n",
      "Iteration  189 => Loss: 68.770447\n",
      "Iteration  190 => Loss: 68.700347\n",
      "Iteration  191 => Loss: 68.630447\n",
      "Iteration  192 => Loss: 68.560747\n",
      "Iteration  193 => Loss: 68.491247\n",
      "Iteration  194 => Loss: 68.421947\n",
      "Iteration  195 => Loss: 68.352847\n",
      "Iteration  196 => Loss: 68.283947\n",
      "Iteration  197 => Loss: 68.215247\n",
      "Iteration  198 => Loss: 68.146747\n",
      "Iteration  199 => Loss: 68.078447\n",
      "Iteration  200 => Loss: 68.010347\n",
      "Iteration  201 => Loss: 68.007853\n",
      "Iteration  202 => Loss: 67.937420\n",
      "Iteration  203 => Loss: 67.867187\n",
      "Iteration  204 => Loss: 67.797153\n",
      "Iteration  205 => Loss: 67.727320\n",
      "Iteration  206 => Loss: 67.657687\n",
      "Iteration  207 => Loss: 67.588253\n",
      "Iteration  208 => Loss: 67.519020\n",
      "Iteration  209 => Loss: 67.449987\n",
      "Iteration  210 => Loss: 67.381153\n",
      "Iteration  211 => Loss: 67.312520\n",
      "Iteration  212 => Loss: 67.244087\n",
      "Iteration  213 => Loss: 67.175853\n",
      "Iteration  214 => Loss: 67.107820\n",
      "Iteration  215 => Loss: 67.039987\n",
      "Iteration  216 => Loss: 66.972353\n",
      "Iteration  217 => Loss: 66.904920\n",
      "Iteration  218 => Loss: 66.837687\n",
      "Iteration  219 => Loss: 66.835887\n",
      "Iteration  220 => Loss: 66.766320\n",
      "Iteration  221 => Loss: 66.696953\n",
      "Iteration  222 => Loss: 66.627787\n",
      "Iteration  223 => Loss: 66.558820\n",
      "Iteration  224 => Loss: 66.490053\n",
      "Iteration  225 => Loss: 66.421487\n",
      "Iteration  226 => Loss: 66.353120\n",
      "Iteration  227 => Loss: 66.284953\n",
      "Iteration  228 => Loss: 66.216987\n",
      "Iteration  229 => Loss: 66.149220\n",
      "Iteration  230 => Loss: 66.081653\n",
      "Iteration  231 => Loss: 66.014287\n",
      "Iteration  232 => Loss: 65.947120\n",
      "Iteration  233 => Loss: 65.880153\n",
      "Iteration  234 => Loss: 65.813387\n",
      "Iteration  235 => Loss: 65.746820\n",
      "Iteration  236 => Loss: 65.680453\n",
      "Iteration  237 => Loss: 65.679347\n",
      "Iteration  238 => Loss: 65.610647\n",
      "Iteration  239 => Loss: 65.542147\n",
      "Iteration  240 => Loss: 65.473847\n",
      "Iteration  241 => Loss: 65.405747\n",
      "Iteration  242 => Loss: 65.337847\n",
      "Iteration  243 => Loss: 65.270147\n",
      "Iteration  244 => Loss: 65.202647\n",
      "Iteration  245 => Loss: 65.135347\n",
      "Iteration  246 => Loss: 65.068247\n",
      "Iteration  247 => Loss: 65.001347\n",
      "Iteration  248 => Loss: 64.934647\n",
      "Iteration  249 => Loss: 64.868147\n",
      "Iteration  250 => Loss: 64.801847\n",
      "Iteration  251 => Loss: 64.735747\n",
      "Iteration  252 => Loss: 64.669847\n",
      "Iteration  253 => Loss: 64.604147\n",
      "Iteration  254 => Loss: 64.538647\n",
      "Iteration  255 => Loss: 64.538233\n",
      "Iteration  256 => Loss: 64.470400\n",
      "Iteration  257 => Loss: 64.402767\n",
      "Iteration  258 => Loss: 64.335333\n",
      "Iteration  259 => Loss: 64.268100\n",
      "Iteration  260 => Loss: 64.201067\n",
      "Iteration  261 => Loss: 64.134233\n",
      "Iteration  262 => Loss: 64.067600\n",
      "Iteration  263 => Loss: 64.001167\n",
      "Iteration  264 => Loss: 63.934933\n",
      "Iteration  265 => Loss: 63.868900\n",
      "Iteration  266 => Loss: 63.803067\n",
      "Iteration  267 => Loss: 63.737433\n",
      "Iteration  268 => Loss: 63.672000\n",
      "Iteration  269 => Loss: 63.606767\n",
      "Iteration  270 => Loss: 63.541733\n",
      "Iteration  271 => Loss: 63.476900\n",
      "Iteration  272 => Loss: 63.412267\n",
      "Iteration  273 => Loss: 63.347833\n",
      "Iteration  274 => Loss: 63.345580\n",
      "Iteration  275 => Loss: 63.278813\n",
      "Iteration  276 => Loss: 63.212247\n",
      "Iteration  277 => Loss: 63.145880\n",
      "Iteration  278 => Loss: 63.079713\n",
      "Iteration  279 => Loss: 63.013747\n",
      "Iteration  280 => Loss: 62.947980\n",
      "Iteration  281 => Loss: 62.882413\n",
      "Iteration  282 => Loss: 62.817047\n",
      "Iteration  283 => Loss: 62.751880\n",
      "Iteration  284 => Loss: 62.686913\n",
      "Iteration  285 => Loss: 62.622147\n",
      "Iteration  286 => Loss: 62.557580\n",
      "Iteration  287 => Loss: 62.493213\n",
      "Iteration  288 => Loss: 62.429047\n",
      "Iteration  289 => Loss: 62.365080\n",
      "Iteration  290 => Loss: 62.301313\n",
      "Iteration  291 => Loss: 62.237747\n",
      "Iteration  292 => Loss: 62.236187\n",
      "Iteration  293 => Loss: 62.170287\n",
      "Iteration  294 => Loss: 62.104587\n",
      "Iteration  295 => Loss: 62.039087\n",
      "Iteration  296 => Loss: 61.973787\n",
      "Iteration  297 => Loss: 61.908687\n",
      "Iteration  298 => Loss: 61.843787\n",
      "Iteration  299 => Loss: 61.779087\n",
      "Iteration  300 => Loss: 61.714587\n",
      "Iteration  301 => Loss: 61.650287\n",
      "Iteration  302 => Loss: 61.586187\n",
      "Iteration  303 => Loss: 61.522287\n",
      "Iteration  304 => Loss: 61.458587\n",
      "Iteration  305 => Loss: 61.395087\n",
      "Iteration  306 => Loss: 61.331787\n",
      "Iteration  307 => Loss: 61.268687\n",
      "Iteration  308 => Loss: 61.205787\n",
      "Iteration  309 => Loss: 61.143087\n",
      "Iteration  310 => Loss: 61.142220\n",
      "Iteration  311 => Loss: 61.077187\n",
      "Iteration  312 => Loss: 61.012353\n",
      "Iteration  313 => Loss: 60.947720\n",
      "Iteration  314 => Loss: 60.883287\n",
      "Iteration  315 => Loss: 60.819053\n",
      "Iteration  316 => Loss: 60.755020\n",
      "Iteration  317 => Loss: 60.691187\n",
      "Iteration  318 => Loss: 60.627553\n",
      "Iteration  319 => Loss: 60.564120\n",
      "Iteration  320 => Loss: 60.500887\n",
      "Iteration  321 => Loss: 60.437853\n",
      "Iteration  322 => Loss: 60.375020\n",
      "Iteration  323 => Loss: 60.312387\n",
      "Iteration  324 => Loss: 60.249953\n",
      "Iteration  325 => Loss: 60.187720\n",
      "Iteration  326 => Loss: 60.125687\n",
      "Iteration  327 => Loss: 60.063853\n",
      "Iteration  328 => Loss: 60.063680\n",
      "Iteration  329 => Loss: 59.999513\n",
      "Iteration  330 => Loss: 59.935547\n",
      "Iteration  331 => Loss: 59.871780\n",
      "Iteration  332 => Loss: 59.808213\n",
      "Iteration  333 => Loss: 59.744847\n",
      "Iteration  334 => Loss: 59.681680\n",
      "Iteration  335 => Loss: 59.618713\n",
      "Iteration  336 => Loss: 59.555947\n",
      "Iteration  337 => Loss: 59.493380\n",
      "Iteration  338 => Loss: 59.431013\n",
      "Iteration  339 => Loss: 59.368847\n",
      "Iteration  340 => Loss: 59.306880\n",
      "Iteration  341 => Loss: 59.245113\n",
      "Iteration  342 => Loss: 59.183547\n",
      "Iteration  343 => Loss: 59.122180\n",
      "Iteration  344 => Loss: 59.061013\n",
      "Iteration  345 => Loss: 59.000047\n",
      "Iteration  346 => Loss: 58.939280\n",
      "Iteration  347 => Loss: 58.937267\n",
      "Iteration  348 => Loss: 58.874167\n",
      "Iteration  349 => Loss: 58.811267\n",
      "Iteration  350 => Loss: 58.748567\n",
      "Iteration  351 => Loss: 58.686067\n",
      "Iteration  352 => Loss: 58.623767\n",
      "Iteration  353 => Loss: 58.561667\n",
      "Iteration  354 => Loss: 58.499767\n",
      "Iteration  355 => Loss: 58.438067\n",
      "Iteration  356 => Loss: 58.376567\n",
      "Iteration  357 => Loss: 58.315267\n",
      "Iteration  358 => Loss: 58.254167\n",
      "Iteration  359 => Loss: 58.193267\n",
      "Iteration  360 => Loss: 58.132567\n",
      "Iteration  361 => Loss: 58.072067\n",
      "Iteration  362 => Loss: 58.011767\n",
      "Iteration  363 => Loss: 57.951667\n",
      "Iteration  364 => Loss: 57.891767\n",
      "Iteration  365 => Loss: 57.890447\n",
      "Iteration  366 => Loss: 57.828213\n",
      "Iteration  367 => Loss: 57.766180\n",
      "Iteration  368 => Loss: 57.704347\n",
      "Iteration  369 => Loss: 57.642713\n",
      "Iteration  370 => Loss: 57.581280\n",
      "Iteration  371 => Loss: 57.520047\n",
      "Iteration  372 => Loss: 57.459013\n",
      "Iteration  373 => Loss: 57.398180\n",
      "Iteration  374 => Loss: 57.337547\n",
      "Iteration  375 => Loss: 57.277113\n",
      "Iteration  376 => Loss: 57.216880\n",
      "Iteration  377 => Loss: 57.156847\n",
      "Iteration  378 => Loss: 57.097013\n",
      "Iteration  379 => Loss: 57.037380\n",
      "Iteration  380 => Loss: 56.977947\n",
      "Iteration  381 => Loss: 56.918713\n",
      "Iteration  382 => Loss: 56.859680\n",
      "Iteration  383 => Loss: 56.859053\n",
      "Iteration  384 => Loss: 56.797687\n",
      "Iteration  385 => Loss: 56.736520\n",
      "Iteration  386 => Loss: 56.675553\n",
      "Iteration  387 => Loss: 56.614787\n",
      "Iteration  388 => Loss: 56.554220\n",
      "Iteration  389 => Loss: 56.493853\n",
      "Iteration  390 => Loss: 56.433687\n",
      "Iteration  391 => Loss: 56.373720\n",
      "Iteration  392 => Loss: 56.313953\n",
      "Iteration  393 => Loss: 56.254387\n",
      "Iteration  394 => Loss: 56.195020\n",
      "Iteration  395 => Loss: 56.135853\n",
      "Iteration  396 => Loss: 56.076887\n",
      "Iteration  397 => Loss: 56.018120\n",
      "Iteration  398 => Loss: 55.959553\n",
      "Iteration  399 => Loss: 55.901187\n",
      "Iteration  400 => Loss: 55.843020\n",
      "Iteration  401 => Loss: 55.785053\n",
      "Iteration  402 => Loss: 55.782587\n",
      "Iteration  403 => Loss: 55.722287\n",
      "Iteration  404 => Loss: 55.662187\n",
      "Iteration  405 => Loss: 55.602287\n",
      "Iteration  406 => Loss: 55.542587\n",
      "Iteration  407 => Loss: 55.483087\n",
      "Iteration  408 => Loss: 55.423787\n",
      "Iteration  409 => Loss: 55.364687\n",
      "Iteration  410 => Loss: 55.305787\n",
      "Iteration  411 => Loss: 55.247087\n",
      "Iteration  412 => Loss: 55.188587\n",
      "Iteration  413 => Loss: 55.130287\n",
      "Iteration  414 => Loss: 55.072187\n",
      "Iteration  415 => Loss: 55.014287\n",
      "Iteration  416 => Loss: 54.956587\n",
      "Iteration  417 => Loss: 54.899087\n",
      "Iteration  418 => Loss: 54.841787\n",
      "Iteration  419 => Loss: 54.784687\n",
      "Iteration  420 => Loss: 54.782913\n",
      "Iteration  421 => Loss: 54.723480\n",
      "Iteration  422 => Loss: 54.664247\n",
      "Iteration  423 => Loss: 54.605213\n",
      "Iteration  424 => Loss: 54.546380\n",
      "Iteration  425 => Loss: 54.487747\n",
      "Iteration  426 => Loss: 54.429313\n",
      "Iteration  427 => Loss: 54.371080\n",
      "Iteration  428 => Loss: 54.313047\n",
      "Iteration  429 => Loss: 54.255213\n",
      "Iteration  430 => Loss: 54.197580\n",
      "Iteration  431 => Loss: 54.140147\n",
      "Iteration  432 => Loss: 54.082913\n",
      "Iteration  433 => Loss: 54.025880\n",
      "Iteration  434 => Loss: 53.969047\n",
      "Iteration  435 => Loss: 53.912413\n",
      "Iteration  436 => Loss: 53.855980\n",
      "Iteration  437 => Loss: 53.799747\n",
      "Iteration  438 => Loss: 53.798667\n",
      "Iteration  439 => Loss: 53.740100\n",
      "Iteration  440 => Loss: 53.681733\n",
      "Iteration  441 => Loss: 53.623567\n",
      "Iteration  442 => Loss: 53.565600\n",
      "Iteration  443 => Loss: 53.507833\n",
      "Iteration  444 => Loss: 53.450267\n",
      "Iteration  445 => Loss: 53.392900\n",
      "Iteration  446 => Loss: 53.335733\n",
      "Iteration  447 => Loss: 53.278767\n",
      "Iteration  448 => Loss: 53.222000\n",
      "Iteration  449 => Loss: 53.165433\n",
      "Iteration  450 => Loss: 53.109067\n",
      "Iteration  451 => Loss: 53.052900\n",
      "Iteration  452 => Loss: 52.996933\n",
      "Iteration  453 => Loss: 52.941167\n",
      "Iteration  454 => Loss: 52.885600\n",
      "Iteration  455 => Loss: 52.830233\n",
      "Iteration  456 => Loss: 52.829847\n",
      "Iteration  457 => Loss: 52.772147\n",
      "Iteration  458 => Loss: 52.714647\n",
      "Iteration  459 => Loss: 52.657347\n",
      "Iteration  460 => Loss: 52.600247\n",
      "Iteration  461 => Loss: 52.543347\n",
      "Iteration  462 => Loss: 52.486647\n",
      "Iteration  463 => Loss: 52.430147\n",
      "Iteration  464 => Loss: 52.373847\n",
      "Iteration  465 => Loss: 52.317747\n",
      "Iteration  466 => Loss: 52.261847\n",
      "Iteration  467 => Loss: 52.206147\n",
      "Iteration  468 => Loss: 52.150647\n",
      "Iteration  469 => Loss: 52.095347\n",
      "Iteration  470 => Loss: 52.040247\n",
      "Iteration  471 => Loss: 51.985347\n",
      "Iteration  472 => Loss: 51.930647\n",
      "Iteration  473 => Loss: 51.876147\n",
      "Iteration  474 => Loss: 51.821847\n",
      "Iteration  475 => Loss: 51.819620\n",
      "Iteration  476 => Loss: 51.762987\n",
      "Iteration  477 => Loss: 51.706553\n",
      "Iteration  478 => Loss: 51.650320\n",
      "Iteration  479 => Loss: 51.594287\n",
      "Iteration  480 => Loss: 51.538453\n",
      "Iteration  481 => Loss: 51.482820\n",
      "Iteration  482 => Loss: 51.427387\n",
      "Iteration  483 => Loss: 51.372153\n",
      "Iteration  484 => Loss: 51.317120\n",
      "Iteration  485 => Loss: 51.262287\n",
      "Iteration  486 => Loss: 51.207653\n",
      "Iteration  487 => Loss: 51.153220\n",
      "Iteration  488 => Loss: 51.098987\n",
      "Iteration  489 => Loss: 51.044953\n",
      "Iteration  490 => Loss: 50.991120\n",
      "Iteration  491 => Loss: 50.937487\n",
      "Iteration  492 => Loss: 50.884053\n",
      "Iteration  493 => Loss: 50.882520\n",
      "Iteration  494 => Loss: 50.826753\n",
      "Iteration  495 => Loss: 50.771187\n",
      "Iteration  496 => Loss: 50.715820\n",
      "Iteration  497 => Loss: 50.660653\n",
      "Iteration  498 => Loss: 50.605687\n",
      "Iteration  499 => Loss: 50.550920\n",
      "Iteration  500 => Loss: 50.496353\n",
      "Iteration  501 => Loss: 50.441987\n",
      "Iteration  502 => Loss: 50.387820\n",
      "Iteration  503 => Loss: 50.333853\n",
      "Iteration  504 => Loss: 50.280087\n",
      "Iteration  505 => Loss: 50.226520\n",
      "Iteration  506 => Loss: 50.173153\n",
      "Iteration  507 => Loss: 50.119987\n",
      "Iteration  508 => Loss: 50.067020\n",
      "Iteration  509 => Loss: 50.014253\n",
      "Iteration  510 => Loss: 49.961687\n",
      "Iteration  511 => Loss: 49.960847\n",
      "Iteration  512 => Loss: 49.905947\n",
      "Iteration  513 => Loss: 49.851247\n",
      "Iteration  514 => Loss: 49.796747\n",
      "Iteration  515 => Loss: 49.742447\n",
      "Iteration  516 => Loss: 49.688347\n",
      "Iteration  517 => Loss: 49.634447\n",
      "Iteration  518 => Loss: 49.580747\n",
      "Iteration  519 => Loss: 49.527247\n",
      "Iteration  520 => Loss: 49.473947\n",
      "Iteration  521 => Loss: 49.420847\n",
      "Iteration  522 => Loss: 49.367947\n",
      "Iteration  523 => Loss: 49.315247\n",
      "Iteration  524 => Loss: 49.262747\n",
      "Iteration  525 => Loss: 49.210447\n",
      "Iteration  526 => Loss: 49.158347\n",
      "Iteration  527 => Loss: 49.106447\n",
      "Iteration  528 => Loss: 49.054747\n",
      "Iteration  529 => Loss: 49.054600\n",
      "Iteration  530 => Loss: 49.000567\n",
      "Iteration  531 => Loss: 48.946733\n",
      "Iteration  532 => Loss: 48.893100\n",
      "Iteration  533 => Loss: 48.839667\n",
      "Iteration  534 => Loss: 48.786433\n",
      "Iteration  535 => Loss: 48.733400\n",
      "Iteration  536 => Loss: 48.680567\n",
      "Iteration  537 => Loss: 48.627933\n",
      "Iteration  538 => Loss: 48.575500\n",
      "Iteration  539 => Loss: 48.523267\n",
      "Iteration  540 => Loss: 48.471233\n",
      "Iteration  541 => Loss: 48.419400\n",
      "Iteration  542 => Loss: 48.367767\n",
      "Iteration  543 => Loss: 48.316333\n",
      "Iteration  544 => Loss: 48.265100\n",
      "Iteration  545 => Loss: 48.214067\n",
      "Iteration  546 => Loss: 48.163233\n",
      "Iteration  547 => Loss: 48.112600\n",
      "Iteration  548 => Loss: 48.110613\n",
      "Iteration  549 => Loss: 48.057647\n",
      "Iteration  550 => Loss: 48.004880\n",
      "Iteration  551 => Loss: 47.952313\n",
      "Iteration  552 => Loss: 47.899947\n",
      "Iteration  553 => Loss: 47.847780\n",
      "Iteration  554 => Loss: 47.795813\n",
      "Iteration  555 => Loss: 47.744047\n",
      "Iteration  556 => Loss: 47.692480\n",
      "Iteration  557 => Loss: 47.641113\n",
      "Iteration  558 => Loss: 47.589947\n",
      "Iteration  559 => Loss: 47.538980\n",
      "Iteration  560 => Loss: 47.488213\n",
      "Iteration  561 => Loss: 47.437647\n",
      "Iteration  562 => Loss: 47.387280\n",
      "Iteration  563 => Loss: 47.337113\n",
      "Iteration  564 => Loss: 47.287147\n",
      "Iteration  565 => Loss: 47.237380\n",
      "Iteration  566 => Loss: 47.236087\n",
      "Iteration  567 => Loss: 47.183987\n",
      "Iteration  568 => Loss: 47.132087\n",
      "Iteration  569 => Loss: 47.080387\n",
      "Iteration  570 => Loss: 47.028887\n",
      "Iteration  571 => Loss: 46.977587\n",
      "Iteration  572 => Loss: 46.926487\n",
      "Iteration  573 => Loss: 46.875587\n",
      "Iteration  574 => Loss: 46.824887\n",
      "Iteration  575 => Loss: 46.774387\n",
      "Iteration  576 => Loss: 46.724087\n",
      "Iteration  577 => Loss: 46.673987\n",
      "Iteration  578 => Loss: 46.624087\n",
      "Iteration  579 => Loss: 46.574387\n",
      "Iteration  580 => Loss: 46.524887\n",
      "Iteration  581 => Loss: 46.475587\n",
      "Iteration  582 => Loss: 46.426487\n",
      "Iteration  583 => Loss: 46.377587\n",
      "Iteration  584 => Loss: 46.376987\n",
      "Iteration  585 => Loss: 46.325753\n",
      "Iteration  586 => Loss: 46.274720\n",
      "Iteration  587 => Loss: 46.223887\n",
      "Iteration  588 => Loss: 46.173253\n",
      "Iteration  589 => Loss: 46.122820\n",
      "Iteration  590 => Loss: 46.072587\n",
      "Iteration  591 => Loss: 46.022553\n",
      "Iteration  592 => Loss: 45.972720\n",
      "Iteration  593 => Loss: 45.923087\n",
      "Iteration  594 => Loss: 45.873653\n",
      "Iteration  595 => Loss: 45.824420\n",
      "Iteration  596 => Loss: 45.775387\n",
      "Iteration  597 => Loss: 45.726553\n",
      "Iteration  598 => Loss: 45.677920\n",
      "Iteration  599 => Loss: 45.629487\n",
      "Iteration  600 => Loss: 45.581253\n",
      "Iteration  601 => Loss: 45.533220\n",
      "Iteration  602 => Loss: 45.485387\n",
      "Iteration  603 => Loss: 45.482947\n",
      "Iteration  604 => Loss: 45.432780\n",
      "Iteration  605 => Loss: 45.382813\n",
      "Iteration  606 => Loss: 45.333047\n",
      "Iteration  607 => Loss: 45.283480\n",
      "Iteration  608 => Loss: 45.234113\n",
      "Iteration  609 => Loss: 45.184947\n",
      "Iteration  610 => Loss: 45.135980\n",
      "Iteration  611 => Loss: 45.087213\n",
      "Iteration  612 => Loss: 45.038647\n",
      "Iteration  613 => Loss: 44.990280\n",
      "Iteration  614 => Loss: 44.942113\n",
      "Iteration  615 => Loss: 44.894147\n",
      "Iteration  616 => Loss: 44.846380\n",
      "Iteration  617 => Loss: 44.798813\n",
      "Iteration  618 => Loss: 44.751447\n",
      "Iteration  619 => Loss: 44.704280\n",
      "Iteration  620 => Loss: 44.657313\n",
      "Iteration  621 => Loss: 44.655567\n",
      "Iteration  622 => Loss: 44.606267\n",
      "Iteration  623 => Loss: 44.557167\n",
      "Iteration  624 => Loss: 44.508267\n",
      "Iteration  625 => Loss: 44.459567\n",
      "Iteration  626 => Loss: 44.411067\n",
      "Iteration  627 => Loss: 44.362767\n",
      "Iteration  628 => Loss: 44.314667\n",
      "Iteration  629 => Loss: 44.266767\n",
      "Iteration  630 => Loss: 44.219067\n",
      "Iteration  631 => Loss: 44.171567\n",
      "Iteration  632 => Loss: 44.124267\n",
      "Iteration  633 => Loss: 44.077167\n",
      "Iteration  634 => Loss: 44.030267\n",
      "Iteration  635 => Loss: 43.983567\n",
      "Iteration  636 => Loss: 43.937067\n",
      "Iteration  637 => Loss: 43.890767\n",
      "Iteration  638 => Loss: 43.844667\n",
      "Iteration  639 => Loss: 43.843613\n",
      "Iteration  640 => Loss: 43.795180\n",
      "Iteration  641 => Loss: 43.746947\n",
      "Iteration  642 => Loss: 43.698913\n",
      "Iteration  643 => Loss: 43.651080\n",
      "Iteration  644 => Loss: 43.603447\n",
      "Iteration  645 => Loss: 43.556013\n",
      "Iteration  646 => Loss: 43.508780\n",
      "Iteration  647 => Loss: 43.461747\n",
      "Iteration  648 => Loss: 43.414913\n",
      "Iteration  649 => Loss: 43.368280\n",
      "Iteration  650 => Loss: 43.321847\n",
      "Iteration  651 => Loss: 43.275613\n",
      "Iteration  652 => Loss: 43.229580\n",
      "Iteration  653 => Loss: 43.183747\n",
      "Iteration  654 => Loss: 43.138113\n",
      "Iteration  655 => Loss: 43.092680\n",
      "Iteration  656 => Loss: 43.047447\n",
      "Iteration  657 => Loss: 43.047087\n",
      "Iteration  658 => Loss: 42.999520\n",
      "Iteration  659 => Loss: 42.952153\n",
      "Iteration  660 => Loss: 42.904987\n",
      "Iteration  661 => Loss: 42.858020\n",
      "Iteration  662 => Loss: 42.811253\n",
      "Iteration  663 => Loss: 42.764687\n",
      "Iteration  664 => Loss: 42.718320\n",
      "Iteration  665 => Loss: 42.672153\n",
      "Iteration  666 => Loss: 42.626187\n",
      "Iteration  667 => Loss: 42.580420\n",
      "Iteration  668 => Loss: 42.534853\n",
      "Iteration  669 => Loss: 42.489487\n",
      "Iteration  670 => Loss: 42.444320\n",
      "Iteration  671 => Loss: 42.399353\n",
      "Iteration  672 => Loss: 42.354587\n",
      "Iteration  673 => Loss: 42.310020\n",
      "Iteration  674 => Loss: 42.265653\n",
      "Iteration  675 => Loss: 42.221487\n",
      "Iteration  676 => Loss: 42.219287\n",
      "Iteration  677 => Loss: 42.172787\n",
      "Iteration  678 => Loss: 42.126487\n",
      "Iteration  679 => Loss: 42.080387\n",
      "Iteration  680 => Loss: 42.034487\n",
      "Iteration  681 => Loss: 41.988787\n",
      "Iteration  682 => Loss: 41.943287\n",
      "Iteration  683 => Loss: 41.897987\n",
      "Iteration  684 => Loss: 41.852887\n",
      "Iteration  685 => Loss: 41.807987\n",
      "Iteration  686 => Loss: 41.763287\n",
      "Iteration  687 => Loss: 41.718787\n",
      "Iteration  688 => Loss: 41.674487\n",
      "Iteration  689 => Loss: 41.630387\n",
      "Iteration  690 => Loss: 41.586487\n",
      "Iteration  691 => Loss: 41.542787\n",
      "Iteration  692 => Loss: 41.499287\n",
      "Iteration  693 => Loss: 41.455987\n",
      "Iteration  694 => Loss: 41.454480\n",
      "Iteration  695 => Loss: 41.408847\n",
      "Iteration  696 => Loss: 41.363413\n",
      "Iteration  697 => Loss: 41.318180\n",
      "Iteration  698 => Loss: 41.273147\n",
      "Iteration  699 => Loss: 41.228313\n",
      "Iteration  700 => Loss: 41.183680\n",
      "Iteration  701 => Loss: 41.139247\n",
      "Iteration  702 => Loss: 41.095013\n",
      "Iteration  703 => Loss: 41.050980\n",
      "Iteration  704 => Loss: 41.007147\n",
      "Iteration  705 => Loss: 40.963513\n",
      "Iteration  706 => Loss: 40.920080\n",
      "Iteration  707 => Loss: 40.876847\n",
      "Iteration  708 => Loss: 40.833813\n",
      "Iteration  709 => Loss: 40.790980\n",
      "Iteration  710 => Loss: 40.748347\n",
      "Iteration  711 => Loss: 40.705913\n",
      "Iteration  712 => Loss: 40.705100\n",
      "Iteration  713 => Loss: 40.660333\n",
      "Iteration  714 => Loss: 40.615767\n",
      "Iteration  715 => Loss: 40.571400\n",
      "Iteration  716 => Loss: 40.527233\n",
      "Iteration  717 => Loss: 40.483267\n",
      "Iteration  718 => Loss: 40.439500\n",
      "Iteration  719 => Loss: 40.395933\n",
      "Iteration  720 => Loss: 40.352567\n",
      "Iteration  721 => Loss: 40.309400\n",
      "Iteration  722 => Loss: 40.266433\n",
      "Iteration  723 => Loss: 40.223667\n",
      "Iteration  724 => Loss: 40.181100\n",
      "Iteration  725 => Loss: 40.138733\n",
      "Iteration  726 => Loss: 40.096567\n",
      "Iteration  727 => Loss: 40.054600\n",
      "Iteration  728 => Loss: 40.012833\n",
      "Iteration  729 => Loss: 39.971267\n",
      "Iteration  730 => Loss: 39.971147\n",
      "Iteration  731 => Loss: 39.927247\n",
      "Iteration  732 => Loss: 39.883547\n",
      "Iteration  733 => Loss: 39.840047\n",
      "Iteration  734 => Loss: 39.796747\n",
      "Iteration  735 => Loss: 39.753647\n",
      "Iteration  736 => Loss: 39.710747\n",
      "Iteration  737 => Loss: 39.668047\n",
      "Iteration  738 => Loss: 39.625547\n",
      "Iteration  739 => Loss: 39.583247\n",
      "Iteration  740 => Loss: 39.541147\n",
      "Iteration  741 => Loss: 39.499247\n",
      "Iteration  742 => Loss: 39.457547\n",
      "Iteration  743 => Loss: 39.416047\n",
      "Iteration  744 => Loss: 39.374747\n",
      "Iteration  745 => Loss: 39.333647\n",
      "Iteration  746 => Loss: 39.292747\n",
      "Iteration  747 => Loss: 39.252047\n",
      "Iteration  748 => Loss: 39.211547\n",
      "Iteration  749 => Loss: 39.209587\n",
      "Iteration  750 => Loss: 39.166753\n",
      "Iteration  751 => Loss: 39.124120\n",
      "Iteration  752 => Loss: 39.081687\n",
      "Iteration  753 => Loss: 39.039453\n",
      "Iteration  754 => Loss: 38.997420\n",
      "Iteration  755 => Loss: 38.955587\n",
      "Iteration  756 => Loss: 38.913953\n",
      "Iteration  757 => Loss: 38.872520\n",
      "Iteration  758 => Loss: 38.831287\n",
      "Iteration  759 => Loss: 38.790253\n",
      "Iteration  760 => Loss: 38.749420\n",
      "Iteration  761 => Loss: 38.708787\n",
      "Iteration  762 => Loss: 38.668353\n",
      "Iteration  763 => Loss: 38.628120\n",
      "Iteration  764 => Loss: 38.588087\n",
      "Iteration  765 => Loss: 38.548253\n",
      "Iteration  766 => Loss: 38.508620\n",
      "Iteration  767 => Loss: 38.507353\n",
      "Iteration  768 => Loss: 38.465387\n",
      "Iteration  769 => Loss: 38.423620\n",
      "Iteration  770 => Loss: 38.382053\n",
      "Iteration  771 => Loss: 38.340687\n",
      "Iteration  772 => Loss: 38.299520\n",
      "Iteration  773 => Loss: 38.258553\n",
      "Iteration  774 => Loss: 38.217787\n",
      "Iteration  775 => Loss: 38.177220\n",
      "Iteration  776 => Loss: 38.136853\n",
      "Iteration  777 => Loss: 38.096687\n",
      "Iteration  778 => Loss: 38.056720\n",
      "Iteration  779 => Loss: 38.016953\n",
      "Iteration  780 => Loss: 37.977387\n",
      "Iteration  781 => Loss: 37.938020\n",
      "Iteration  782 => Loss: 37.898853\n",
      "Iteration  783 => Loss: 37.859887\n",
      "Iteration  784 => Loss: 37.821120\n",
      "Iteration  785 => Loss: 37.820547\n",
      "Iteration  786 => Loss: 37.779447\n",
      "Iteration  787 => Loss: 37.738547\n",
      "Iteration  788 => Loss: 37.697847\n",
      "Iteration  789 => Loss: 37.657347\n",
      "Iteration  790 => Loss: 37.617047\n",
      "Iteration  791 => Loss: 37.576947\n",
      "Iteration  792 => Loss: 37.537047\n",
      "Iteration  793 => Loss: 37.497347\n",
      "Iteration  794 => Loss: 37.457847\n",
      "Iteration  795 => Loss: 37.418547\n",
      "Iteration  796 => Loss: 37.379447\n",
      "Iteration  797 => Loss: 37.340547\n",
      "Iteration  798 => Loss: 37.301847\n",
      "Iteration  799 => Loss: 37.263347\n",
      "Iteration  800 => Loss: 37.225047\n",
      "Iteration  801 => Loss: 37.186947\n",
      "Iteration  802 => Loss: 37.149047\n",
      "Iteration  803 => Loss: 37.111347\n",
      "Iteration  804 => Loss: 37.108933\n",
      "Iteration  805 => Loss: 37.068900\n",
      "Iteration  806 => Loss: 37.029067\n",
      "Iteration  807 => Loss: 36.989433\n",
      "Iteration  808 => Loss: 36.950000\n",
      "Iteration  809 => Loss: 36.910767\n",
      "Iteration  810 => Loss: 36.871733\n",
      "Iteration  811 => Loss: 36.832900\n",
      "Iteration  812 => Loss: 36.794267\n",
      "Iteration  813 => Loss: 36.755833\n",
      "Iteration  814 => Loss: 36.717600\n",
      "Iteration  815 => Loss: 36.679567\n",
      "Iteration  816 => Loss: 36.641733\n",
      "Iteration  817 => Loss: 36.604100\n",
      "Iteration  818 => Loss: 36.566667\n",
      "Iteration  819 => Loss: 36.529433\n",
      "Iteration  820 => Loss: 36.492400\n",
      "Iteration  821 => Loss: 36.455567\n",
      "Iteration  822 => Loss: 36.453847\n",
      "Iteration  823 => Loss: 36.414680\n",
      "Iteration  824 => Loss: 36.375713\n",
      "Iteration  825 => Loss: 36.336947\n",
      "Iteration  826 => Loss: 36.298380\n",
      "Iteration  827 => Loss: 36.260013\n",
      "Iteration  828 => Loss: 36.221847\n",
      "Iteration  829 => Loss: 36.183880\n",
      "Iteration  830 => Loss: 36.146113\n",
      "Iteration  831 => Loss: 36.108547\n",
      "Iteration  832 => Loss: 36.071180\n",
      "Iteration  833 => Loss: 36.034013\n",
      "Iteration  834 => Loss: 35.997047\n",
      "Iteration  835 => Loss: 35.960280\n",
      "Iteration  836 => Loss: 35.923713\n",
      "Iteration  837 => Loss: 35.887347\n",
      "Iteration  838 => Loss: 35.851180\n",
      "Iteration  839 => Loss: 35.815213\n",
      "Iteration  840 => Loss: 35.814187\n",
      "Iteration  841 => Loss: 35.775887\n",
      "Iteration  842 => Loss: 35.737787\n",
      "Iteration  843 => Loss: 35.699887\n",
      "Iteration  844 => Loss: 35.662187\n",
      "Iteration  845 => Loss: 35.624687\n",
      "Iteration  846 => Loss: 35.587387\n",
      "Iteration  847 => Loss: 35.550287\n",
      "Iteration  848 => Loss: 35.513387\n",
      "Iteration  849 => Loss: 35.476687\n",
      "Iteration  850 => Loss: 35.440187\n",
      "Iteration  851 => Loss: 35.403887\n",
      "Iteration  852 => Loss: 35.367787\n",
      "Iteration  853 => Loss: 35.331887\n",
      "Iteration  854 => Loss: 35.296187\n",
      "Iteration  855 => Loss: 35.260687\n",
      "Iteration  856 => Loss: 35.225387\n",
      "Iteration  857 => Loss: 35.190287\n",
      "Iteration  858 => Loss: 35.189953\n",
      "Iteration  859 => Loss: 35.152520\n",
      "Iteration  860 => Loss: 35.115287\n",
      "Iteration  861 => Loss: 35.078253\n",
      "Iteration  862 => Loss: 35.041420\n",
      "Iteration  863 => Loss: 35.004787\n",
      "Iteration  864 => Loss: 34.968353\n",
      "Iteration  865 => Loss: 34.932120\n",
      "Iteration  866 => Loss: 34.896087\n",
      "Iteration  867 => Loss: 34.860253\n",
      "Iteration  868 => Loss: 34.824620\n",
      "Iteration  869 => Loss: 34.789187\n",
      "Iteration  870 => Loss: 34.753953\n",
      "Iteration  871 => Loss: 34.718920\n",
      "Iteration  872 => Loss: 34.684087\n",
      "Iteration  873 => Loss: 34.649453\n",
      "Iteration  874 => Loss: 34.615020\n",
      "Iteration  875 => Loss: 34.580787\n",
      "Iteration  876 => Loss: 34.546753\n",
      "Iteration  877 => Loss: 34.544580\n",
      "Iteration  878 => Loss: 34.508213\n",
      "Iteration  879 => Loss: 34.472047\n",
      "Iteration  880 => Loss: 34.436080\n",
      "Iteration  881 => Loss: 34.400313\n",
      "Iteration  882 => Loss: 34.364747\n",
      "Iteration  883 => Loss: 34.329380\n",
      "Iteration  884 => Loss: 34.294213\n",
      "Iteration  885 => Loss: 34.259247\n",
      "Iteration  886 => Loss: 34.224480\n",
      "Iteration  887 => Loss: 34.189913\n",
      "Iteration  888 => Loss: 34.155547\n",
      "Iteration  889 => Loss: 34.121380\n",
      "Iteration  890 => Loss: 34.087413\n",
      "Iteration  891 => Loss: 34.053647\n",
      "Iteration  892 => Loss: 34.020080\n",
      "Iteration  893 => Loss: 33.986713\n",
      "Iteration  894 => Loss: 33.953547\n",
      "Iteration  895 => Loss: 33.952067\n",
      "Iteration  896 => Loss: 33.916567\n",
      "Iteration  897 => Loss: 33.881267\n",
      "Iteration  898 => Loss: 33.846167\n",
      "Iteration  899 => Loss: 33.811267\n",
      "Iteration  900 => Loss: 33.776567\n",
      "Iteration  901 => Loss: 33.742067\n",
      "Iteration  902 => Loss: 33.707767\n",
      "Iteration  903 => Loss: 33.673667\n",
      "Iteration  904 => Loss: 33.639767\n",
      "Iteration  905 => Loss: 33.606067\n",
      "Iteration  906 => Loss: 33.572567\n",
      "Iteration  907 => Loss: 33.539267\n",
      "Iteration  908 => Loss: 33.506167\n",
      "Iteration  909 => Loss: 33.473267\n",
      "Iteration  910 => Loss: 33.440567\n",
      "Iteration  911 => Loss: 33.408067\n",
      "Iteration  912 => Loss: 33.375767\n",
      "Iteration  913 => Loss: 33.374980\n",
      "Iteration  914 => Loss: 33.340347\n",
      "Iteration  915 => Loss: 33.305913\n",
      "Iteration  916 => Loss: 33.271680\n",
      "Iteration  917 => Loss: 33.237647\n",
      "Iteration  918 => Loss: 33.203813\n",
      "Iteration  919 => Loss: 33.170180\n",
      "Iteration  920 => Loss: 33.136747\n",
      "Iteration  921 => Loss: 33.103513\n",
      "Iteration  922 => Loss: 33.070480\n",
      "Iteration  923 => Loss: 33.037647\n",
      "Iteration  924 => Loss: 33.005013\n",
      "Iteration  925 => Loss: 32.972580\n",
      "Iteration  926 => Loss: 32.940347\n",
      "Iteration  927 => Loss: 32.908313\n",
      "Iteration  928 => Loss: 32.876480\n",
      "Iteration  929 => Loss: 32.844847\n",
      "Iteration  930 => Loss: 32.813413\n",
      "Iteration  931 => Loss: 32.813320\n",
      "Iteration  932 => Loss: 32.779553\n",
      "Iteration  933 => Loss: 32.745987\n",
      "Iteration  934 => Loss: 32.712620\n",
      "Iteration  935 => Loss: 32.679453\n",
      "Iteration  936 => Loss: 32.646487\n",
      "Iteration  937 => Loss: 32.613720\n",
      "Iteration  938 => Loss: 32.581153\n",
      "Iteration  939 => Loss: 32.548787\n",
      "Iteration  940 => Loss: 32.516620\n",
      "Iteration  941 => Loss: 32.484653\n",
      "Iteration  942 => Loss: 32.452887\n",
      "Iteration  943 => Loss: 32.421320\n",
      "Iteration  944 => Loss: 32.389953\n",
      "Iteration  945 => Loss: 32.358787\n",
      "Iteration  946 => Loss: 32.327820\n",
      "Iteration  947 => Loss: 32.297053\n",
      "Iteration  948 => Loss: 32.266487\n",
      "Iteration  949 => Loss: 32.236120\n",
      "Iteration  950 => Loss: 32.234187\n",
      "Iteration  951 => Loss: 32.201487\n",
      "Iteration  952 => Loss: 32.168987\n",
      "Iteration  953 => Loss: 32.136687\n",
      "Iteration  954 => Loss: 32.104587\n",
      "Iteration  955 => Loss: 32.072687\n",
      "Iteration  956 => Loss: 32.040987\n",
      "Iteration  957 => Loss: 32.009487\n",
      "Iteration  958 => Loss: 31.978187\n",
      "Iteration  959 => Loss: 31.947087\n",
      "Iteration  960 => Loss: 31.916187\n",
      "Iteration  961 => Loss: 31.885487\n",
      "Iteration  962 => Loss: 31.854987\n",
      "Iteration  963 => Loss: 31.824687\n",
      "Iteration  964 => Loss: 31.794587\n",
      "Iteration  965 => Loss: 31.764687\n",
      "Iteration  966 => Loss: 31.734987\n",
      "Iteration  967 => Loss: 31.705487\n",
      "Iteration  968 => Loss: 31.704247\n",
      "Iteration  969 => Loss: 31.672413\n",
      "Iteration  970 => Loss: 31.640780\n",
      "Iteration  971 => Loss: 31.609347\n",
      "Iteration  972 => Loss: 31.578113\n",
      "Iteration  973 => Loss: 31.547080\n",
      "Iteration  974 => Loss: 31.516247\n",
      "Iteration  975 => Loss: 31.485613\n",
      "Iteration  976 => Loss: 31.455180\n",
      "Iteration  977 => Loss: 31.424947\n",
      "Iteration  978 => Loss: 31.394913\n",
      "Iteration  979 => Loss: 31.365080\n",
      "Iteration  980 => Loss: 31.335447\n",
      "Iteration  981 => Loss: 31.306013\n",
      "Iteration  982 => Loss: 31.276780\n",
      "Iteration  983 => Loss: 31.247747\n",
      "Iteration  984 => Loss: 31.218913\n",
      "Iteration  985 => Loss: 31.190280\n",
      "Iteration  986 => Loss: 31.189733\n",
      "Iteration  987 => Loss: 31.158767\n",
      "Iteration  988 => Loss: 31.128000\n",
      "Iteration  989 => Loss: 31.097433\n",
      "Iteration  990 => Loss: 31.067067\n",
      "Iteration  991 => Loss: 31.036900\n",
      "Iteration  992 => Loss: 31.006933\n",
      "Iteration  993 => Loss: 30.977167\n",
      "Iteration  994 => Loss: 30.947600\n",
      "Iteration  995 => Loss: 30.918233\n",
      "Iteration  996 => Loss: 30.889067\n",
      "Iteration  997 => Loss: 30.860100\n",
      "Iteration  998 => Loss: 30.831333\n",
      "Iteration  999 => Loss: 30.802767\n",
      "Iteration 1000 => Loss: 30.774400\n",
      "Iteration 1001 => Loss: 30.746233\n",
      "Iteration 1002 => Loss: 30.718267\n",
      "Iteration 1003 => Loss: 30.690500\n",
      "Iteration 1004 => Loss: 30.662933\n",
      "Iteration 1005 => Loss: 30.660547\n",
      "Iteration 1006 => Loss: 30.630647\n",
      "Iteration 1007 => Loss: 30.600947\n",
      "Iteration 1008 => Loss: 30.571447\n",
      "Iteration 1009 => Loss: 30.542147\n",
      "Iteration 1010 => Loss: 30.513047\n",
      "Iteration 1011 => Loss: 30.484147\n",
      "Iteration 1012 => Loss: 30.455447\n",
      "Iteration 1013 => Loss: 30.426947\n",
      "Iteration 1014 => Loss: 30.398647\n",
      "Iteration 1015 => Loss: 30.370547\n",
      "Iteration 1016 => Loss: 30.342647\n",
      "Iteration 1017 => Loss: 30.314947\n",
      "Iteration 1018 => Loss: 30.287447\n",
      "Iteration 1019 => Loss: 30.260147\n",
      "Iteration 1020 => Loss: 30.233047\n",
      "Iteration 1021 => Loss: 30.206147\n",
      "Iteration 1022 => Loss: 30.179447\n",
      "Iteration 1023 => Loss: 30.177753\n",
      "Iteration 1024 => Loss: 30.148720\n",
      "Iteration 1025 => Loss: 30.119887\n",
      "Iteration 1026 => Loss: 30.091253\n",
      "Iteration 1027 => Loss: 30.062820\n",
      "Iteration 1028 => Loss: 30.034587\n",
      "Iteration 1029 => Loss: 30.006553\n",
      "Iteration 1030 => Loss: 29.978720\n",
      "Iteration 1031 => Loss: 29.951087\n",
      "Iteration 1032 => Loss: 29.923653\n",
      "Iteration 1033 => Loss: 29.896420\n",
      "Iteration 1034 => Loss: 29.869387\n",
      "Iteration 1035 => Loss: 29.842553\n",
      "Iteration 1036 => Loss: 29.815920\n",
      "Iteration 1037 => Loss: 29.789487\n",
      "Iteration 1038 => Loss: 29.763253\n",
      "Iteration 1039 => Loss: 29.737220\n",
      "Iteration 1040 => Loss: 29.711387\n",
      "Iteration 1041 => Loss: 29.710387\n",
      "Iteration 1042 => Loss: 29.682220\n",
      "Iteration 1043 => Loss: 29.654253\n",
      "Iteration 1044 => Loss: 29.626487\n",
      "Iteration 1045 => Loss: 29.598920\n",
      "Iteration 1046 => Loss: 29.571553\n",
      "Iteration 1047 => Loss: 29.544387\n",
      "Iteration 1048 => Loss: 29.517420\n",
      "Iteration 1049 => Loss: 29.490653\n",
      "Iteration 1050 => Loss: 29.464087\n",
      "Iteration 1051 => Loss: 29.437720\n",
      "Iteration 1052 => Loss: 29.411553\n",
      "Iteration 1053 => Loss: 29.385587\n",
      "Iteration 1054 => Loss: 29.359820\n",
      "Iteration 1055 => Loss: 29.334253\n",
      "Iteration 1056 => Loss: 29.308887\n",
      "Iteration 1057 => Loss: 29.283720\n",
      "Iteration 1058 => Loss: 29.258753\n",
      "Iteration 1059 => Loss: 29.258447\n",
      "Iteration 1060 => Loss: 29.231147\n",
      "Iteration 1061 => Loss: 29.204047\n",
      "Iteration 1062 => Loss: 29.177147\n",
      "Iteration 1063 => Loss: 29.150447\n",
      "Iteration 1064 => Loss: 29.123947\n",
      "Iteration 1065 => Loss: 29.097647\n",
      "Iteration 1066 => Loss: 29.071547\n",
      "Iteration 1067 => Loss: 29.045647\n",
      "Iteration 1068 => Loss: 29.019947\n",
      "Iteration 1069 => Loss: 28.994447\n",
      "Iteration 1070 => Loss: 28.969147\n",
      "Iteration 1071 => Loss: 28.944047\n",
      "Iteration 1072 => Loss: 28.919147\n",
      "Iteration 1073 => Loss: 28.894447\n",
      "Iteration 1074 => Loss: 28.869947\n",
      "Iteration 1075 => Loss: 28.845647\n",
      "Iteration 1076 => Loss: 28.821547\n",
      "Iteration 1077 => Loss: 28.797647\n",
      "Iteration 1078 => Loss: 28.795500\n",
      "Iteration 1079 => Loss: 28.769267\n",
      "Iteration 1080 => Loss: 28.743233\n",
      "Iteration 1081 => Loss: 28.717400\n",
      "Iteration 1082 => Loss: 28.691767\n",
      "Iteration 1083 => Loss: 28.666333\n",
      "Iteration 1084 => Loss: 28.641100\n",
      "Iteration 1085 => Loss: 28.616067\n",
      "Iteration 1086 => Loss: 28.591233\n",
      "Iteration 1087 => Loss: 28.566600\n",
      "Iteration 1088 => Loss: 28.542167\n",
      "Iteration 1089 => Loss: 28.517933\n",
      "Iteration 1090 => Loss: 28.493900\n",
      "Iteration 1091 => Loss: 28.470067\n",
      "Iteration 1092 => Loss: 28.446433\n",
      "Iteration 1093 => Loss: 28.423000\n",
      "Iteration 1094 => Loss: 28.399767\n",
      "Iteration 1095 => Loss: 28.376733\n",
      "Iteration 1096 => Loss: 28.375280\n",
      "Iteration 1097 => Loss: 28.349913\n",
      "Iteration 1098 => Loss: 28.324747\n",
      "Iteration 1099 => Loss: 28.299780\n",
      "Iteration 1100 => Loss: 28.275013\n",
      "Iteration 1101 => Loss: 28.250447\n",
      "Iteration 1102 => Loss: 28.226080\n",
      "Iteration 1103 => Loss: 28.201913\n",
      "Iteration 1104 => Loss: 28.177947\n",
      "Iteration 1105 => Loss: 28.154180\n",
      "Iteration 1106 => Loss: 28.130613\n",
      "Iteration 1107 => Loss: 28.107247\n",
      "Iteration 1108 => Loss: 28.084080\n",
      "Iteration 1109 => Loss: 28.061113\n",
      "Iteration 1110 => Loss: 28.038347\n",
      "Iteration 1111 => Loss: 28.015780\n",
      "Iteration 1112 => Loss: 27.993413\n",
      "Iteration 1113 => Loss: 27.971247\n",
      "Iteration 1114 => Loss: 27.970487\n",
      "Iteration 1115 => Loss: 27.945987\n",
      "Iteration 1116 => Loss: 27.921687\n",
      "Iteration 1117 => Loss: 27.897587\n",
      "Iteration 1118 => Loss: 27.873687\n",
      "Iteration 1119 => Loss: 27.849987\n",
      "Iteration 1120 => Loss: 27.826487\n",
      "Iteration 1121 => Loss: 27.803187\n",
      "Iteration 1122 => Loss: 27.780087\n",
      "Iteration 1123 => Loss: 27.757187\n",
      "Iteration 1124 => Loss: 27.734487\n",
      "Iteration 1125 => Loss: 27.711987\n",
      "Iteration 1126 => Loss: 27.689687\n",
      "Iteration 1127 => Loss: 27.667587\n",
      "Iteration 1128 => Loss: 27.645687\n",
      "Iteration 1129 => Loss: 27.623987\n",
      "Iteration 1130 => Loss: 27.602487\n",
      "Iteration 1131 => Loss: 27.581187\n",
      "Iteration 1132 => Loss: 27.581120\n",
      "Iteration 1133 => Loss: 27.557487\n",
      "Iteration 1134 => Loss: 27.534053\n",
      "Iteration 1135 => Loss: 27.510820\n",
      "Iteration 1136 => Loss: 27.487787\n",
      "Iteration 1137 => Loss: 27.464953\n",
      "Iteration 1138 => Loss: 27.442320\n",
      "Iteration 1139 => Loss: 27.419887\n",
      "Iteration 1140 => Loss: 27.397653\n",
      "Iteration 1141 => Loss: 27.375620\n",
      "Iteration 1142 => Loss: 27.353787\n",
      "Iteration 1143 => Loss: 27.332153\n",
      "Iteration 1144 => Loss: 27.310720\n",
      "Iteration 1145 => Loss: 27.289487\n",
      "Iteration 1146 => Loss: 27.268453\n",
      "Iteration 1147 => Loss: 27.247620\n",
      "Iteration 1148 => Loss: 27.226987\n",
      "Iteration 1149 => Loss: 27.206553\n",
      "Iteration 1150 => Loss: 27.186320\n",
      "Iteration 1151 => Loss: 27.184413\n",
      "Iteration 1152 => Loss: 27.161847\n",
      "Iteration 1153 => Loss: 27.139480\n",
      "Iteration 1154 => Loss: 27.117313\n",
      "Iteration 1155 => Loss: 27.095347\n",
      "Iteration 1156 => Loss: 27.073580\n",
      "Iteration 1157 => Loss: 27.052013\n",
      "Iteration 1158 => Loss: 27.030647\n",
      "Iteration 1159 => Loss: 27.009480\n",
      "Iteration 1160 => Loss: 26.988513\n",
      "Iteration 1161 => Loss: 26.967747\n",
      "Iteration 1162 => Loss: 26.947180\n",
      "Iteration 1163 => Loss: 26.926813\n",
      "Iteration 1164 => Loss: 26.906647\n",
      "Iteration 1165 => Loss: 26.886680\n",
      "Iteration 1166 => Loss: 26.866913\n",
      "Iteration 1167 => Loss: 26.847347\n",
      "Iteration 1168 => Loss: 26.827980\n",
      "Iteration 1169 => Loss: 26.826767\n",
      "Iteration 1170 => Loss: 26.805067\n",
      "Iteration 1171 => Loss: 26.783567\n",
      "Iteration 1172 => Loss: 26.762267\n",
      "Iteration 1173 => Loss: 26.741167\n",
      "Iteration 1174 => Loss: 26.720267\n",
      "Iteration 1175 => Loss: 26.699567\n",
      "Iteration 1176 => Loss: 26.679067\n",
      "Iteration 1177 => Loss: 26.658767\n",
      "Iteration 1178 => Loss: 26.638667\n",
      "Iteration 1179 => Loss: 26.618767\n",
      "Iteration 1180 => Loss: 26.599067\n",
      "Iteration 1181 => Loss: 26.579567\n",
      "Iteration 1182 => Loss: 26.560267\n",
      "Iteration 1183 => Loss: 26.541167\n",
      "Iteration 1184 => Loss: 26.522267\n",
      "Iteration 1185 => Loss: 26.503567\n",
      "Iteration 1186 => Loss: 26.485067\n",
      "Iteration 1187 => Loss: 26.484547\n",
      "Iteration 1188 => Loss: 26.463713\n",
      "Iteration 1189 => Loss: 26.443080\n",
      "Iteration 1190 => Loss: 26.422647\n",
      "Iteration 1191 => Loss: 26.402413\n",
      "Iteration 1192 => Loss: 26.382380\n",
      "Iteration 1193 => Loss: 26.362547\n",
      "Iteration 1194 => Loss: 26.342913\n",
      "Iteration 1195 => Loss: 26.323480\n",
      "Iteration 1196 => Loss: 26.304247\n",
      "Iteration 1197 => Loss: 26.285213\n",
      "Iteration 1198 => Loss: 26.266380\n",
      "Iteration 1199 => Loss: 26.247747\n",
      "Iteration 1200 => Loss: 26.229313\n",
      "Iteration 1201 => Loss: 26.211080\n",
      "Iteration 1202 => Loss: 26.193047\n",
      "Iteration 1203 => Loss: 26.175213\n",
      "Iteration 1204 => Loss: 26.157580\n",
      "Iteration 1205 => Loss: 26.140147\n",
      "Iteration 1206 => Loss: 26.137787\n",
      "Iteration 1207 => Loss: 26.118020\n",
      "Iteration 1208 => Loss: 26.098453\n",
      "Iteration 1209 => Loss: 26.079087\n",
      "Iteration 1210 => Loss: 26.059920\n",
      "Iteration 1211 => Loss: 26.040953\n",
      "Iteration 1212 => Loss: 26.022187\n",
      "Iteration 1213 => Loss: 26.003620\n",
      "Iteration 1214 => Loss: 25.985253\n",
      "Iteration 1215 => Loss: 25.967087\n",
      "Iteration 1216 => Loss: 25.949120\n",
      "Iteration 1217 => Loss: 25.931353\n",
      "Iteration 1218 => Loss: 25.913787\n",
      "Iteration 1219 => Loss: 25.896420\n",
      "Iteration 1220 => Loss: 25.879253\n",
      "Iteration 1221 => Loss: 25.862287\n",
      "Iteration 1222 => Loss: 25.845520\n",
      "Iteration 1223 => Loss: 25.828953\n",
      "Iteration 1224 => Loss: 25.827287\n",
      "Iteration 1225 => Loss: 25.808387\n",
      "Iteration 1226 => Loss: 25.789687\n",
      "Iteration 1227 => Loss: 25.771187\n",
      "Iteration 1228 => Loss: 25.752887\n",
      "Iteration 1229 => Loss: 25.734787\n",
      "Iteration 1230 => Loss: 25.716887\n",
      "Iteration 1231 => Loss: 25.699187\n",
      "Iteration 1232 => Loss: 25.681687\n",
      "Iteration 1233 => Loss: 25.664387\n",
      "Iteration 1234 => Loss: 25.647287\n",
      "Iteration 1235 => Loss: 25.630387\n",
      "Iteration 1236 => Loss: 25.613687\n",
      "Iteration 1237 => Loss: 25.597187\n",
      "Iteration 1238 => Loss: 25.580887\n",
      "Iteration 1239 => Loss: 25.564787\n",
      "Iteration 1240 => Loss: 25.548887\n",
      "Iteration 1241 => Loss: 25.533187\n",
      "Iteration 1242 => Loss: 25.532213\n",
      "Iteration 1243 => Loss: 25.514180\n",
      "Iteration 1244 => Loss: 25.496347\n",
      "Iteration 1245 => Loss: 25.478713\n",
      "Iteration 1246 => Loss: 25.461280\n",
      "Iteration 1247 => Loss: 25.444047\n",
      "Iteration 1248 => Loss: 25.427013\n",
      "Iteration 1249 => Loss: 25.410180\n",
      "Iteration 1250 => Loss: 25.393547\n",
      "Iteration 1251 => Loss: 25.377113\n",
      "Iteration 1252 => Loss: 25.360880\n",
      "Iteration 1253 => Loss: 25.344847\n",
      "Iteration 1254 => Loss: 25.329013\n",
      "Iteration 1255 => Loss: 25.313380\n",
      "Iteration 1256 => Loss: 25.297947\n",
      "Iteration 1257 => Loss: 25.282713\n",
      "Iteration 1258 => Loss: 25.267680\n",
      "Iteration 1259 => Loss: 25.252847\n",
      "Iteration 1260 => Loss: 25.252567\n",
      "Iteration 1261 => Loss: 25.235400\n",
      "Iteration 1262 => Loss: 25.218433\n",
      "Iteration 1263 => Loss: 25.201667\n",
      "Iteration 1264 => Loss: 25.185100\n",
      "Iteration 1265 => Loss: 25.168733\n",
      "Iteration 1266 => Loss: 25.152567\n",
      "Iteration 1267 => Loss: 25.136600\n",
      "Iteration 1268 => Loss: 25.120833\n",
      "Iteration 1269 => Loss: 25.105267\n",
      "Iteration 1270 => Loss: 25.089900\n",
      "Iteration 1271 => Loss: 25.074733\n",
      "Iteration 1272 => Loss: 25.059767\n",
      "Iteration 1273 => Loss: 25.045000\n",
      "Iteration 1274 => Loss: 25.030433\n",
      "Iteration 1275 => Loss: 25.016067\n",
      "Iteration 1276 => Loss: 25.001900\n",
      "Iteration 1277 => Loss: 24.987933\n",
      "Iteration 1278 => Loss: 24.974167\n",
      "Iteration 1279 => Loss: 24.972047\n",
      "Iteration 1280 => Loss: 24.955947\n",
      "Iteration 1281 => Loss: 24.940047\n",
      "Iteration 1282 => Loss: 24.924347\n",
      "Iteration 1283 => Loss: 24.908847\n",
      "Iteration 1284 => Loss: 24.893547\n",
      "Iteration 1285 => Loss: 24.878447\n",
      "Iteration 1286 => Loss: 24.863547\n",
      "Iteration 1287 => Loss: 24.848847\n",
      "Iteration 1288 => Loss: 24.834347\n",
      "Iteration 1289 => Loss: 24.820047\n",
      "Iteration 1290 => Loss: 24.805947\n",
      "Iteration 1291 => Loss: 24.792047\n",
      "Iteration 1292 => Loss: 24.778347\n",
      "Iteration 1293 => Loss: 24.764847\n",
      "Iteration 1294 => Loss: 24.751547\n",
      "Iteration 1295 => Loss: 24.738447\n",
      "Iteration 1296 => Loss: 24.725547\n",
      "Iteration 1297 => Loss: 24.724120\n",
      "Iteration 1298 => Loss: 24.708887\n",
      "Iteration 1299 => Loss: 24.693853\n",
      "Iteration 1300 => Loss: 24.679020\n",
      "Iteration 1301 => Loss: 24.664387\n",
      "Iteration 1302 => Loss: 24.649953\n",
      "Iteration 1303 => Loss: 24.635720\n",
      "Iteration 1304 => Loss: 24.621687\n",
      "Iteration 1305 => Loss: 24.607853\n",
      "Iteration 1306 => Loss: 24.594220\n",
      "Iteration 1307 => Loss: 24.580787\n",
      "Iteration 1308 => Loss: 24.567553\n",
      "Iteration 1309 => Loss: 24.554520\n",
      "Iteration 1310 => Loss: 24.541687\n",
      "Iteration 1311 => Loss: 24.529053\n",
      "Iteration 1312 => Loss: 24.516620\n",
      "Iteration 1313 => Loss: 24.504387\n",
      "Iteration 1314 => Loss: 24.492353\n",
      "Iteration 1315 => Loss: 24.491620\n",
      "Iteration 1316 => Loss: 24.477253\n",
      "Iteration 1317 => Loss: 24.463087\n",
      "Iteration 1318 => Loss: 24.449120\n",
      "Iteration 1319 => Loss: 24.435353\n",
      "Iteration 1320 => Loss: 24.421787\n",
      "Iteration 1321 => Loss: 24.408420\n",
      "Iteration 1322 => Loss: 24.395253\n",
      "Iteration 1323 => Loss: 24.382287\n",
      "Iteration 1324 => Loss: 24.369520\n",
      "Iteration 1325 => Loss: 24.356953\n",
      "Iteration 1326 => Loss: 24.344587\n",
      "Iteration 1327 => Loss: 24.332420\n",
      "Iteration 1328 => Loss: 24.320453\n",
      "Iteration 1329 => Loss: 24.308687\n",
      "Iteration 1330 => Loss: 24.297120\n",
      "Iteration 1331 => Loss: 24.285753\n",
      "Iteration 1332 => Loss: 24.274587\n",
      "Iteration 1333 => Loss: 24.274547\n",
      "Iteration 1334 => Loss: 24.261047\n",
      "Iteration 1335 => Loss: 24.247747\n",
      "Iteration 1336 => Loss: 24.234647\n",
      "Iteration 1337 => Loss: 24.221747\n",
      "Iteration 1338 => Loss: 24.209047\n",
      "Iteration 1339 => Loss: 24.196547\n",
      "Iteration 1340 => Loss: 24.184247\n",
      "Iteration 1341 => Loss: 24.172147\n",
      "Iteration 1342 => Loss: 24.160247\n",
      "Iteration 1343 => Loss: 24.148547\n",
      "Iteration 1344 => Loss: 24.137047\n",
      "Iteration 1345 => Loss: 24.125747\n",
      "Iteration 1346 => Loss: 24.114647\n",
      "Iteration 1347 => Loss: 24.103747\n",
      "Iteration 1348 => Loss: 24.093047\n",
      "Iteration 1349 => Loss: 24.082547\n",
      "Iteration 1350 => Loss: 24.072247\n",
      "Iteration 1351 => Loss: 24.062147\n",
      "Iteration 1352 => Loss: 24.060267\n",
      "Iteration 1353 => Loss: 24.047833\n",
      "Iteration 1354 => Loss: 24.035600\n",
      "Iteration 1355 => Loss: 24.023567\n",
      "Iteration 1356 => Loss: 24.011733\n",
      "Iteration 1357 => Loss: 24.000100\n",
      "Iteration 1358 => Loss: 23.988667\n",
      "Iteration 1359 => Loss: 23.977433\n",
      "Iteration 1360 => Loss: 23.966400\n",
      "Iteration 1361 => Loss: 23.955567\n",
      "Iteration 1362 => Loss: 23.944933\n",
      "Iteration 1363 => Loss: 23.934500\n",
      "Iteration 1364 => Loss: 23.924267\n",
      "Iteration 1365 => Loss: 23.914233\n",
      "Iteration 1366 => Loss: 23.904400\n",
      "Iteration 1367 => Loss: 23.894767\n",
      "Iteration 1368 => Loss: 23.885333\n",
      "Iteration 1369 => Loss: 23.876100\n",
      "Iteration 1370 => Loss: 23.874913\n",
      "Iteration 1371 => Loss: 23.863347\n",
      "Iteration 1372 => Loss: 23.851980\n",
      "Iteration 1373 => Loss: 23.840813\n",
      "Iteration 1374 => Loss: 23.829847\n",
      "Iteration 1375 => Loss: 23.819080\n",
      "Iteration 1376 => Loss: 23.808513\n",
      "Iteration 1377 => Loss: 23.798147\n",
      "Iteration 1378 => Loss: 23.787980\n",
      "Iteration 1379 => Loss: 23.778013\n",
      "Iteration 1380 => Loss: 23.768247\n",
      "Iteration 1381 => Loss: 23.758680\n",
      "Iteration 1382 => Loss: 23.749313\n",
      "Iteration 1383 => Loss: 23.740147\n",
      "Iteration 1384 => Loss: 23.731180\n",
      "Iteration 1385 => Loss: 23.722413\n",
      "Iteration 1386 => Loss: 23.713847\n",
      "Iteration 1387 => Loss: 23.705480\n",
      "Iteration 1388 => Loss: 23.704987\n",
      "Iteration 1389 => Loss: 23.694287\n",
      "Iteration 1390 => Loss: 23.683787\n",
      "Iteration 1391 => Loss: 23.673487\n",
      "Iteration 1392 => Loss: 23.663387\n",
      "Iteration 1393 => Loss: 23.653487\n",
      "Iteration 1394 => Loss: 23.643787\n",
      "Iteration 1395 => Loss: 23.634287\n",
      "Iteration 1396 => Loss: 23.624987\n",
      "Iteration 1397 => Loss: 23.615887\n",
      "Iteration 1398 => Loss: 23.606987\n",
      "Iteration 1399 => Loss: 23.598287\n",
      "Iteration 1400 => Loss: 23.589787\n",
      "Iteration 1401 => Loss: 23.581487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1402 => Loss: 23.573387\n",
      "Iteration 1403 => Loss: 23.565487\n",
      "Iteration 1404 => Loss: 23.557787\n",
      "Iteration 1405 => Loss: 23.550287\n",
      "Iteration 1406 => Loss: 23.542987\n",
      "Iteration 1407 => Loss: 23.540653\n",
      "Iteration 1408 => Loss: 23.531020\n",
      "Iteration 1409 => Loss: 23.521587\n",
      "Iteration 1410 => Loss: 23.512353\n",
      "Iteration 1411 => Loss: 23.503320\n",
      "Iteration 1412 => Loss: 23.494487\n",
      "Iteration 1413 => Loss: 23.485853\n",
      "Iteration 1414 => Loss: 23.477420\n",
      "Iteration 1415 => Loss: 23.469187\n",
      "Iteration 1416 => Loss: 23.461153\n",
      "Iteration 1417 => Loss: 23.453320\n",
      "Iteration 1418 => Loss: 23.445687\n",
      "Iteration 1419 => Loss: 23.438253\n",
      "Iteration 1420 => Loss: 23.431020\n",
      "Iteration 1421 => Loss: 23.423987\n",
      "Iteration 1422 => Loss: 23.417153\n",
      "Iteration 1423 => Loss: 23.410520\n",
      "Iteration 1424 => Loss: 23.404087\n",
      "Iteration 1425 => Loss: 23.402447\n",
      "Iteration 1426 => Loss: 23.393680\n",
      "Iteration 1427 => Loss: 23.385113\n",
      "Iteration 1428 => Loss: 23.376747\n",
      "Iteration 1429 => Loss: 23.368580\n",
      "Iteration 1430 => Loss: 23.360613\n",
      "Iteration 1431 => Loss: 23.352847\n",
      "Iteration 1432 => Loss: 23.345280\n",
      "Iteration 1433 => Loss: 23.337913\n",
      "Iteration 1434 => Loss: 23.330747\n",
      "Iteration 1435 => Loss: 23.323780\n",
      "Iteration 1436 => Loss: 23.317013\n",
      "Iteration 1437 => Loss: 23.310447\n",
      "Iteration 1438 => Loss: 23.304080\n",
      "Iteration 1439 => Loss: 23.297913\n",
      "Iteration 1440 => Loss: 23.291947\n",
      "Iteration 1441 => Loss: 23.286180\n",
      "Iteration 1442 => Loss: 23.280613\n",
      "Iteration 1443 => Loss: 23.279667\n",
      "Iteration 1444 => Loss: 23.271767\n",
      "Iteration 1445 => Loss: 23.264067\n",
      "Iteration 1446 => Loss: 23.256567\n",
      "Iteration 1447 => Loss: 23.249267\n",
      "Iteration 1448 => Loss: 23.242167\n",
      "Iteration 1449 => Loss: 23.235267\n",
      "Iteration 1450 => Loss: 23.228567\n",
      "Iteration 1451 => Loss: 23.222067\n",
      "Iteration 1452 => Loss: 23.215767\n",
      "Iteration 1453 => Loss: 23.209667\n",
      "Iteration 1454 => Loss: 23.203767\n",
      "Iteration 1455 => Loss: 23.198067\n",
      "Iteration 1456 => Loss: 23.192567\n",
      "Iteration 1457 => Loss: 23.187267\n",
      "Iteration 1458 => Loss: 23.182167\n",
      "Iteration 1459 => Loss: 23.177267\n",
      "Iteration 1460 => Loss: 23.172567\n",
      "Iteration 1461 => Loss: 23.172313\n",
      "Iteration 1462 => Loss: 23.165280\n",
      "Iteration 1463 => Loss: 23.158447\n",
      "Iteration 1464 => Loss: 23.151813\n",
      "Iteration 1465 => Loss: 23.145380\n",
      "Iteration 1466 => Loss: 23.139147\n",
      "Iteration 1467 => Loss: 23.133113\n",
      "Iteration 1468 => Loss: 23.127280\n",
      "Iteration 1469 => Loss: 23.121647\n",
      "Iteration 1470 => Loss: 23.116213\n",
      "Iteration 1471 => Loss: 23.110980\n",
      "Iteration 1472 => Loss: 23.105947\n",
      "Iteration 1473 => Loss: 23.101113\n",
      "Iteration 1474 => Loss: 23.096480\n",
      "Iteration 1475 => Loss: 23.092047\n",
      "Iteration 1476 => Loss: 23.087813\n",
      "Iteration 1477 => Loss: 23.083780\n",
      "Iteration 1478 => Loss: 23.079947\n",
      "Iteration 1479 => Loss: 23.076313\n",
      "Iteration 1480 => Loss: 23.074220\n",
      "Iteration 1481 => Loss: 23.068253\n",
      "Iteration 1482 => Loss: 23.062487\n",
      "Iteration 1483 => Loss: 23.056920\n",
      "Iteration 1484 => Loss: 23.051553\n",
      "Iteration 1485 => Loss: 23.046387\n",
      "Iteration 1486 => Loss: 23.041420\n",
      "Iteration 1487 => Loss: 23.036653\n",
      "Iteration 1488 => Loss: 23.032087\n",
      "Iteration 1489 => Loss: 23.027720\n",
      "Iteration 1490 => Loss: 23.023553\n",
      "Iteration 1491 => Loss: 23.019587\n",
      "Iteration 1492 => Loss: 23.015820\n",
      "Iteration 1493 => Loss: 23.012253\n",
      "Iteration 1494 => Loss: 23.008887\n",
      "Iteration 1495 => Loss: 23.005720\n",
      "Iteration 1496 => Loss: 23.002753\n",
      "Iteration 1497 => Loss: 22.999987\n",
      "Iteration 1498 => Loss: 22.998587\n",
      "Iteration 1499 => Loss: 22.993487\n",
      "Iteration 1500 => Loss: 22.988587\n",
      "Iteration 1501 => Loss: 22.983887\n",
      "Iteration 1502 => Loss: 22.979387\n",
      "Iteration 1503 => Loss: 22.975087\n",
      "Iteration 1504 => Loss: 22.970987\n",
      "Iteration 1505 => Loss: 22.967087\n",
      "Iteration 1506 => Loss: 22.963387\n",
      "Iteration 1507 => Loss: 22.959887\n",
      "Iteration 1508 => Loss: 22.956587\n",
      "Iteration 1509 => Loss: 22.953487\n",
      "Iteration 1510 => Loss: 22.950587\n",
      "Iteration 1511 => Loss: 22.947887\n",
      "Iteration 1512 => Loss: 22.945387\n",
      "Iteration 1513 => Loss: 22.943087\n",
      "Iteration 1514 => Loss: 22.940987\n",
      "Iteration 1515 => Loss: 22.939087\n",
      "Iteration 1516 => Loss: 22.938380\n",
      "Iteration 1517 => Loss: 22.934147\n",
      "Iteration 1518 => Loss: 22.930113\n",
      "Iteration 1519 => Loss: 22.926280\n",
      "Iteration 1520 => Loss: 22.922647\n",
      "Iteration 1521 => Loss: 22.919213\n",
      "Iteration 1522 => Loss: 22.915980\n",
      "Iteration 1523 => Loss: 22.912947\n",
      "Iteration 1524 => Loss: 22.910113\n",
      "Iteration 1525 => Loss: 22.907480\n",
      "Iteration 1526 => Loss: 22.905047\n",
      "Iteration 1527 => Loss: 22.902813\n",
      "Iteration 1528 => Loss: 22.900780\n",
      "Iteration 1529 => Loss: 22.898947\n",
      "Iteration 1530 => Loss: 22.897313\n",
      "Iteration 1531 => Loss: 22.895880\n",
      "Iteration 1532 => Loss: 22.894647\n",
      "Iteration 1533 => Loss: 22.893613\n",
      "Iteration 1534 => Loss: 22.893600\n",
      "Iteration 1535 => Loss: 22.890233\n",
      "Iteration 1536 => Loss: 22.887067\n",
      "Iteration 1537 => Loss: 22.884100\n",
      "Iteration 1538 => Loss: 22.881333\n",
      "Iteration 1539 => Loss: 22.878767\n",
      "Iteration 1540 => Loss: 22.876400\n",
      "Iteration 1541 => Loss: 22.874233\n",
      "Iteration 1542 => Loss: 22.872267\n",
      "Iteration 1543 => Loss: 22.870500\n",
      "Iteration 1544 => Loss: 22.868933\n",
      "Iteration 1545 => Loss: 22.867567\n",
      "Iteration 1546 => Loss: 22.866400\n",
      "Iteration 1547 => Loss: 22.865433\n",
      "Iteration 1548 => Loss: 22.864667\n",
      "Iteration 1549 => Loss: 22.864100\n",
      "Iteration 1550 => Loss: 22.863733\n",
      "Iteration 1551 => Loss: 22.863567\n"
     ]
    }
   ],
   "source": [
    "w, b = train(X, Y, 10000, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! Here are the line's parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1000000000000008"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.929999999999769"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the line visualized over the examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the examples like we did before:\n",
    "plt.xlabel(\"Reservations\", fontsize=20)\n",
    "plt.ylabel(\"Pizzas\", fontsize=20)\n",
    "plt.axis([0, 50, 0, 50])\n",
    "plt.plot(X, Y, \"bo\")\n",
    "\n",
    "# Plot the line:\n",
    "plt.plot([0, 50], [b, predict(50, w, b)], color=\"g\")\n",
    "\n",
    "# Visualize the diagram:\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's use this model to predict how many pizzas we're going to sell if we got 42 reservations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.1299999999998"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reservations = 42\n",
    "predict(reservations, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we should prepare enough dough for about 60 pizzas. We just built a system that learns!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
