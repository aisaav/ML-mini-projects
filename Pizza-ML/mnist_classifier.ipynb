{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An MNIST Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict all the digits in MNIST! Here is the code that downloads the dataset again, in case you didn't run it in the previous Notebook, again no harm in running it again, but you do not have to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run once, to download MNIST\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# Create an 'mnist' directory unless it exists:\n",
    "LOCAL_DIR = './mnist/'\n",
    "if not os.path.exists(LOCAL_DIR):\n",
    "    os.makedirs(LOCAL_DIR)\n",
    "\n",
    "# Download the four MNIST files from the official site:\n",
    "MNIST_SITE = 'http://yann.lecun.com/exdb/mnist/'\n",
    "TRAINING_IMAGES = 'train-images-idx3-ubyte.gz'\n",
    "TRAINING_LABELS = 'train-labels-idx1-ubyte.gz'\n",
    "TEST_IMAGES = 't10k-images-idx3-ubyte.gz'\n",
    "TEST_LABELS = 't10k-labels-idx1-ubyte.gz'\n",
    "\n",
    "urllib.request.urlretrieve(MNIST_SITE + TRAINING_IMAGES, LOCAL_DIR + TRAINING_IMAGES)\n",
    "urllib.request.urlretrieve(MNIST_SITE + TRAINING_LABELS, LOCAL_DIR + TRAINING_LABELS)\n",
    "urllib.request.urlretrieve(MNIST_SITE + TEST_IMAGES, LOCAL_DIR + TEST_IMAGES)\n",
    "urllib.request.urlretrieve(MNIST_SITE + TEST_LABELS, LOCAL_DIR + TEST_LABELS)\n",
    "\n",
    "print(\"Data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here the MNIST image loader again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import struct\n",
    "\n",
    "def load_images(filename):\n",
    "    # Open and unzip the file of images:\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        # Read the header information into a bunch of variables:\n",
    "        _ignored, n_images, image_columns, image_rows = struct.unpack('>IIII', f.read(16))\n",
    "        # Read all the pixels into a long NumPy array:\n",
    "        all_pixels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        # Reshape the array into a matrix where each line is an image:\n",
    "        images_matrix = all_pixels.reshape(n_images, image_columns * image_rows)\n",
    "        # Add a bias column full of 1s as the first column in the matrix\n",
    "        return np.insert(images_matrix, 0, 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60000 images, each 785 elements (1 bias + 28 * 28 pixels):\n",
    "X_train = load_images(\"./mnist/train-images-idx3-ubyte.gz\")\n",
    "\n",
    "# 10000 images, each 785 elements, with the same structure as X_train:\n",
    "X_test = load_images(\"./mnist/t10k-images-idx3-ubyte.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code that loads the MNIST labels has changed since the last module. Now, instead of returning binary labels (\"Is this digit a 4?\"), it returns one hot encoded labels for all digits from 0 to 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(filename):\n",
    "    # Open and unzip the file of images:\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        # Skip the header bytes:\n",
    "        f.read(8)\n",
    "        # Read all the labels into a list:\n",
    "        all_labels = f.read()\n",
    "        # Reshape the list of labels into a one-column matrix:\n",
    "        return np.frombuffer(all_labels, dtype=np.uint8).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(Y):\n",
    "    NUMBER_OF_CLASSES = 10        # One class per digit\n",
    "    NUMBER_OF_LABELS = Y.shape[0] # One label for each row in Y\n",
    "    \n",
    "    # Prepare a matrix of zeros with as many rows as the rows in Y,\n",
    "    # and as many columns as the number of classes:\n",
    "    encoded_labels = np.zeros((NUMBER_OF_LABELS, NUMBER_OF_CLASSES))\n",
    "\n",
    "    # For each row, flip the column that matches the label to 1:\n",
    "    for row in range(NUMBER_OF_LABELS):\n",
    "        label = Y[row]\n",
    "        encoded_labels[row][label] = 1\n",
    "        \n",
    "    return encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 60K labels, each with value 1 if the digit is a five, and 0 otherwise:\n",
    "original_labels = load_labels(\"./mnist/train-labels-idx1-ubyte.gz\")\n",
    "Y_train = one_hot_encode(original_labels)\n",
    "\n",
    "# 10000 labels, with the same encoding as Y_train:\n",
    "Y_test = load_labels(\"./mnist/t10k-labels-idx1-ubyte.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the new labels loader works. Here are the first ten original labels in the training set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5],\n",
       "       [0],\n",
       "       [4],\n",
       "       [1],\n",
       "       [9],\n",
       "       [2],\n",
       "       [1],\n",
       "       [3],\n",
       "       [1],\n",
       "       [4]], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and here are their one hot encoded counterparts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier's code didn't change, but it gained a new `classify` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w):\n",
    "    return sigmoid(np.matmul(X, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(X, w):\n",
    "    # Calculate predictionsâ€“a matrix with one row per\n",
    "    # prediction, each row including ten \"votes of\n",
    "    # confidence\" for the ten digits:\n",
    "    predictions = predict(X, w)\n",
    "    # For each row, pick the index of the highest\n",
    "    # prediction:\n",
    "    labels = np.argmax(predictions, axis=1)\n",
    "    # Reshape the labels to be one column, and as many\n",
    "    # rows as necessary:\n",
    "    return labels.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, Y, w):\n",
    "    predictions = predict(X, w)\n",
    "    first_term = Y * np.log(predictions)\n",
    "    second_term = (1 - Y) * np.log(1 - predictions)\n",
    "    return -np.average(first_term + second_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, Y, w):\n",
    "    return np.matmul(X.T, (predict(X, w) - Y)) / X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, iterations, lr):\n",
    "    w = np.zeros((X.shape[1], Y.shape[1]))\n",
    "    for i in range(iterations):\n",
    "        print(\"Iteration %4d => Loss: %.20f\" % (i, loss(X, Y, w)))\n",
    "        w -= gradient(X, Y, w) * lr\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 => Loss: 0.69314718055994539725\n",
      "Iteration    1 => Loss: 0.84344568750833359694\n",
      "Iteration    2 => Loss: 0.55120474889238757488\n",
      "Iteration    3 => Loss: 0.29568700735936542801\n",
      "Iteration    4 => Loss: 0.18985387657057095634\n",
      "Iteration    5 => Loss: 0.17558289155266745829\n",
      "Iteration    6 => Loss: 0.16748812729262180232\n",
      "Iteration    7 => Loss: 0.16238752434202810560\n",
      "Iteration    8 => Loss: 0.15652805689746651652\n",
      "Iteration    9 => Loss: 0.15292692651055575515\n",
      "Iteration   10 => Loss: 0.14834968500183898232\n",
      "Iteration   11 => Loss: 0.14547390723537276247\n",
      "Iteration   12 => Loss: 0.14187844781439437614\n",
      "Iteration   13 => Loss: 0.13942565669684220397\n",
      "Iteration   14 => Loss: 0.13659350910622258812\n",
      "Iteration   15 => Loss: 0.13445875188347644613\n",
      "Iteration   16 => Loss: 0.13220198232096094793\n",
      "Iteration   17 => Loss: 0.13034634184193591433\n",
      "Iteration   18 => Loss: 0.12851171137662320554\n",
      "Iteration   19 => Loss: 0.12690683151500536940\n",
      "Iteration   20 => Loss: 0.12537827753717958879\n",
      "Iteration   21 => Loss: 0.12398963816638572388\n",
      "Iteration   22 => Loss: 0.12268469039957431710\n",
      "Iteration   23 => Loss: 0.12147405757324779285\n",
      "Iteration   24 => Loss: 0.12033678230704050738\n",
      "Iteration   25 => Loss: 0.11926945710963290925\n",
      "Iteration   26 => Loss: 0.11826267784085117696\n",
      "Iteration   27 => Loss: 0.11731133453159595104\n",
      "Iteration   28 => Loss: 0.11640996430905363457\n",
      "Iteration   29 => Loss: 0.11555433100032615201\n",
      "Iteration   30 => Loss: 0.11474062582531800725\n",
      "Iteration   31 => Loss: 0.11396556120791269395\n",
      "Iteration   32 => Loss: 0.11322622276175958733\n",
      "Iteration   33 => Loss: 0.11252000991663875518\n",
      "Iteration   34 => Loss: 0.11184459015731983145\n",
      "Iteration   35 => Loss: 0.11119785470055272569\n",
      "Iteration   36 => Loss: 0.11057789294728635754\n",
      "Iteration   37 => Loss: 0.10998296534231662147\n",
      "Iteration   38 => Loss: 0.10941148512417755989\n",
      "Iteration   39 => Loss: 0.10886200036823868997\n",
      "Iteration   40 => Loss: 0.10833318007300121733\n",
      "Iteration   41 => Loss: 0.10782380125640525503\n",
      "Iteration   42 => Loss: 0.10733273816533013545\n",
      "Iteration   43 => Loss: 0.10685895254477489891\n",
      "Iteration   44 => Loss: 0.10640148520586770187\n",
      "Iteration   45 => Loss: 0.10595944849463857784\n",
      "Iteration   46 => Loss: 0.10553201965482390812\n",
      "Iteration   47 => Loss: 0.10511843490562006176\n",
      "Iteration   48 => Loss: 0.10471798417442820806\n",
      "Iteration   49 => Loss: 0.10433000638544995264\n",
      "Iteration   50 => Loss: 0.10395388524401659447\n",
      "Iteration   51 => Loss: 0.10358904545227759497\n",
      "Iteration   52 => Loss: 0.10323494930713016104\n",
      "Iteration   53 => Loss: 0.10289109363468196268\n",
      "Iteration   54 => Loss: 0.10255700702317861739\n",
      "Iteration   55 => Loss: 0.10223224732049561447\n",
      "Iteration   56 => Loss: 0.10191639936700630509\n",
      "Iteration   57 => Loss: 0.10160907293812221353\n",
      "Iteration   58 => Loss: 0.10130990087406219202\n",
      "Iteration   59 => Loss: 0.10101853737709637349\n",
      "Iteration   60 => Loss: 0.10073465645889038977\n",
      "Iteration   61 => Loss: 0.10045795052261211056\n",
      "Iteration   62 => Loss: 0.10018812906624541270\n",
      "Iteration   63 => Loss: 0.09992491749509932464\n",
      "Iteration   64 => Loss: 0.09966805603285393278\n",
      "Iteration   65 => Loss: 0.09941729872166252258\n",
      "Iteration   66 => Loss: 0.09917241250286659704\n",
      "Iteration   67 => Loss: 0.09893317637078717641\n",
      "Iteration   68 => Loss: 0.09869938059285654486\n",
      "Iteration   69 => Loss: 0.09847082599005871351\n",
      "Iteration   70 => Loss: 0.09824732327226907980\n",
      "Iteration   71 => Loss: 0.09802869242363640434\n",
      "Iteration   72 => Loss: 0.09781476213363465844\n",
      "Iteration   73 => Loss: 0.09760536926984911266\n",
      "Iteration   74 => Loss: 0.09740035838894509079\n",
      "Iteration   75 => Loss: 0.09719958128261008135\n",
      "Iteration   76 => Loss: 0.09700289655556749990\n",
      "Iteration   77 => Loss: 0.09681016923303324695\n",
      "Iteration   78 => Loss: 0.09662127039523042693\n",
      "Iteration   79 => Loss: 0.09643607683679848719\n",
      "Iteration   80 => Loss: 0.09625447074912732437\n",
      "Iteration   81 => Loss: 0.09607633942382620695\n",
      "Iteration   82 => Loss: 0.09590157497569432032\n",
      "Iteration   83 => Loss: 0.09573007408370425020\n",
      "Iteration   84 => Loss: 0.09556173774863813142\n",
      "Iteration   85 => Loss: 0.09539647106613419192\n",
      "Iteration   86 => Loss: 0.09523418301400408958\n",
      "Iteration   87 => Loss: 0.09507478625278034667\n",
      "Iteration   88 => Loss: 0.09491819693853757733\n",
      "Iteration   89 => Loss: 0.09476433454710976589\n",
      "Iteration   90 => Loss: 0.09461312170889817075\n",
      "Iteration   91 => Loss: 0.09446448405352791955\n",
      "Iteration   92 => Loss: 0.09431835006367185470\n",
      "Iteration   93 => Loss: 0.09417465093741228532\n",
      "Iteration   94 => Loss: 0.09403332045856198362\n",
      "Iteration   95 => Loss: 0.09389429487440836830\n",
      "Iteration   96 => Loss: 0.09375751278038842174\n",
      "Iteration   97 => Loss: 0.09362291501123652659\n",
      "Iteration   98 => Loss: 0.09349044453818355915\n",
      "Iteration   99 => Loss: 0.09336004637181584409\n",
      "Iteration  100 => Loss: 0.09323166747023205192\n",
      "Iteration  101 => Loss: 0.09310525665216144720\n",
      "Iteration  102 => Loss: 0.09298076451473316628\n",
      "Iteration  103 => Loss: 0.09285814335560498012\n",
      "Iteration  104 => Loss: 0.09273734709918497743\n",
      "Iteration  105 => Loss: 0.09261833122669416163\n",
      "Iteration  106 => Loss: 0.09250105270983886852\n",
      "Iteration  107 => Loss: 0.09238546994787481814\n",
      "Iteration  108 => Loss: 0.09227154270786265544\n",
      "Iteration  109 => Loss: 0.09215923206792479838\n",
      "Iteration  110 => Loss: 0.09204850036333060703\n",
      "Iteration  111 => Loss: 0.09193931113524327070\n",
      "Iteration  112 => Loss: 0.09183162908197733953\n",
      "Iteration  113 => Loss: 0.09172542001262304345\n",
      "Iteration  114 => Loss: 0.09162065080290404673\n",
      "Iteration  115 => Loss: 0.09151728935314258628\n",
      "Iteration  116 => Loss: 0.09141530454821612806\n",
      "Iteration  117 => Loss: 0.09131466621939468564\n",
      "Iteration  118 => Loss: 0.09121534510795661910\n",
      "Iteration  119 => Loss: 0.09111731283048600527\n",
      "Iteration  120 => Loss: 0.09102054184576147122\n",
      "Iteration  121 => Loss: 0.09092500542315094803\n",
      "Iteration  122 => Loss: 0.09083067761243340810\n",
      "Iteration  123 => Loss: 0.09073753321497184110\n",
      "Iteration  124 => Loss: 0.09064554775616716353\n",
      "Iteration  125 => Loss: 0.09055469745912726742\n",
      "Iteration  126 => Loss: 0.09046495921948777275\n",
      "Iteration  127 => Loss: 0.09037631058132698791\n",
      "Iteration  128 => Loss: 0.09028872971411891490\n",
      "Iteration  129 => Loss: 0.09020219539067206305\n",
      "Iteration  130 => Loss: 0.09011668696600559636\n",
      "Iteration  131 => Loss: 0.09003218435711582424\n",
      "Iteration  132 => Loss: 0.08994866802358979230\n",
      "Iteration  133 => Loss: 0.08986611894902386821\n",
      "Iteration  134 => Loss: 0.08978451862320953325\n",
      "Iteration  135 => Loss: 0.08970384902504818792\n",
      "Iteration  136 => Loss: 0.08962409260616116535\n",
      "Iteration  137 => Loss: 0.08954523227516143769\n",
      "Iteration  138 => Loss: 0.08946725138255615106\n",
      "Iteration  139 => Loss: 0.08939013370625013832\n",
      "Iteration  140 => Loss: 0.08931386343762312552\n",
      "Iteration  141 => Loss: 0.08923842516815339021\n",
      "Iteration  142 => Loss: 0.08916380387656350215\n",
      "Iteration  143 => Loss: 0.08908998491646412388\n",
      "Iteration  144 => Loss: 0.08901695400447301454\n",
      "Iteration  145 => Loss: 0.08894469720878876717\n",
      "Iteration  146 => Loss: 0.08887320093819803257\n",
      "Iteration  147 => Loss: 0.08880245193149782779\n",
      "Iteration  148 => Loss: 0.08873243724731395832\n",
      "Iteration  149 => Loss: 0.08866314425429877577\n",
      "Iteration  150 => Loss: 0.08859456062169172863\n",
      "Iteration  151 => Loss: 0.08852667431022635813\n",
      "Iteration  152 => Loss: 0.08845947356337004186\n",
      "Iteration  153 => Loss: 0.08839294689888098355\n",
      "Iteration  154 => Loss: 0.08832708310067058366\n",
      "Iteration  155 => Loss: 0.08826187121095645238\n",
      "Iteration  156 => Loss: 0.08819730052269560139\n",
      "Iteration  157 => Loss: 0.08813336057228507436\n",
      "Iteration  158 => Loss: 0.08807004113251948318\n",
      "Iteration  159 => Loss: 0.08800733220579473604\n",
      "Iteration  160 => Loss: 0.08794522401754774343\n",
      "Iteration  161 => Loss: 0.08788370700992308160\n",
      "Iteration  162 => Loss: 0.08782277183565657952\n",
      "Iteration  163 => Loss: 0.08776240935216804429\n",
      "Iteration  164 => Loss: 0.08770261061585415963\n",
      "Iteration  165 => Loss: 0.08764336687657357794\n",
      "Iteration  166 => Loss: 0.08758466957231762773\n",
      "Iteration  167 => Loss: 0.08752651032405744935\n",
      "Iteration  168 => Loss: 0.08746888093076329862\n",
      "Iteration  169 => Loss: 0.08741177336458645641\n",
      "Iteration  170 => Loss: 0.08735517976620010838\n",
      "Iteration  171 => Loss: 0.08729909244029100679\n",
      "Iteration  172 => Loss: 0.08724350385119786222\n",
      "Iteration  173 => Loss: 0.08718840661868948461\n",
      "Iteration  174 => Loss: 0.08713379351387864902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  175 => Loss: 0.08707965745526551060\n",
      "Iteration  176 => Loss: 0.08702599150490665514\n",
      "Iteration  177 => Loss: 0.08697278886470455328\n",
      "Iteration  178 => Loss: 0.08692004287281317187\n",
      "Iteration  179 => Loss: 0.08686774700015559292\n",
      "Iteration  180 => Loss: 0.08681589484704910209\n",
      "Iteration  181 => Loss: 0.08676448013993504071\n",
      "Iteration  182 => Loss: 0.08671349672820746757\n",
      "Iteration  183 => Loss: 0.08666293858113992277\n",
      "Iteration  184 => Loss: 0.08661279978490420139\n",
      "Iteration  185 => Loss: 0.08656307453967934662\n",
      "Iteration  186 => Loss: 0.08651375715684747614\n",
      "Iteration  187 => Loss: 0.08646484205627291697\n",
      "Iteration  188 => Loss: 0.08641632376366235879\n",
      "Iteration  189 => Loss: 0.08636819690800261184\n",
      "Iteration  190 => Loss: 0.08632045621907370736\n",
      "Iteration  191 => Loss: 0.08627309652503444004\n",
      "Iteration  192 => Loss: 0.08622611275007879827\n",
      "Iteration  193 => Loss: 0.08617949991215881345\n",
      "Iteration  194 => Loss: 0.08613325312077418916\n",
      "Iteration  195 => Loss: 0.08608736757482422786\n",
      "Iteration  196 => Loss: 0.08604183856052141643\n",
      "Iteration  197 => Loss: 0.08599666144936415901\n",
      "Iteration  198 => Loss: 0.08595183169616599228\n",
      "Iteration  199 => Loss: 0.08590734483714086711\n"
     ]
    }
   ],
   "source": [
    "w = train(X_train, Y_train, iterations=200, lr=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training is over, the following code calculates the accuracy of the system on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute classifications:\n",
    "classifications = classify(X_test, w)\n",
    "# Count the predictions that match the original labels:\n",
    "matched_predictions = np.count_nonzero(classifications == Y_test)\n",
    "# Calculate the % of matched predictions:\n",
    "matches_percent = 100 * matched_predictions / Y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.32"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches_percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a much shorter version of the same classifier. I removed all the unnecessary code, but it still works great. Don't be surprised if you don't see anything on the screen during training. I removed the logging code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def classify(X, w):\n",
    "    return np.argmax(sigmoid(np.matmul(X, w)), axis=1).reshape(-1, 1)\n",
    "\n",
    "def train(X, Y, iterations, lr):\n",
    "    w = np.zeros((X.shape[1], Y.shape[1]))\n",
    "    for i in range(iterations):\n",
    "        w -= np.matmul(X.T, (sigmoid(np.matmul(X, w)) - Y)) / X.shape[0] * lr\n",
    "    return w\n",
    "\n",
    "w = train(X_train, Y_train, iterations=200, lr=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the final accuracy is still:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.32"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 * np.count_nonzero(classify(X_test, w) == Y_test) / Y_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
