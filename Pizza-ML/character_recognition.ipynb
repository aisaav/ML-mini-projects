{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Recognition\n",
    "\n",
    "This notebook will show how to recognize characters in ML and how to use our binary classifier notebook to help us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell downloads MNIST from the Internet to an `mnist` directory. It takes some time, but you should only have to run it once. However, there is no harm in running it multiple times, if you do it by mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "# Only run once, to download MNIST.\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# Create an 'mnist' directory unless it exists:\n",
    "LOCAL_DIR = './mnist/'\n",
    "if not os.path.exists(LOCAL_DIR):\n",
    "    os.makedirs(LOCAL_DIR)\n",
    "\n",
    "# Download the four MNIST files from the official site:\n",
    "MNIST_SITE = 'http://yann.lecun.com/exdb/mnist/'\n",
    "TRAINING_IMAGES = 'train-images-idx3-ubyte.gz'\n",
    "TRAINING_LABELS = 'train-labels-idx1-ubyte.gz'\n",
    "TEST_IMAGES = 't10k-images-idx3-ubyte.gz'\n",
    "TEST_LABELS = 't10k-labels-idx1-ubyte.gz'\n",
    "\n",
    "urllib.request.urlretrieve(MNIST_SITE + TRAINING_IMAGES, LOCAL_DIR + TRAINING_IMAGES)\n",
    "urllib.request.urlretrieve(MNIST_SITE + TRAINING_LABELS, LOCAL_DIR + TRAINING_LABELS)\n",
    "urllib.request.urlretrieve(MNIST_SITE + TEST_IMAGES, LOCAL_DIR + TEST_IMAGES)\n",
    "urllib.request.urlretrieve(MNIST_SITE + TEST_LABELS, LOCAL_DIR + TEST_LABELS)\n",
    "\n",
    "print(\"Data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here's the code that loads MNIST, starting with the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import struct\n",
    "\n",
    "def load_images(filename):\n",
    "    # Open and unzip the file of images:\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        # Read the header information into a bunch of variables:\n",
    "        _ignored, n_images, image_columns, image_rows = struct.unpack('>IIII', f.read(16))\n",
    "        # Read all the pixels into a long NumPy array:\n",
    "        all_pixels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        # Reshape the array into a matrix where each line is an image:\n",
    "        images_matrix = all_pixels.reshape(n_images, image_columns * image_rows)\n",
    "        # Add a bias column full of 1s as the first column in the matrix\n",
    "        return np.insert(images_matrix, 0, 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60000 images, each 785 elements (1 bias + 28 * 28 pixels)\n",
    "X_train = load_images(\"./mnist/train-images-idx3-ubyte.gz\")\n",
    "\n",
    "# 10000 images, each 785 elements, with the same structure as X_train\n",
    "X_test = load_images(\"./mnist/t10k-images-idx3-ubyte.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that we have a (60000, 785) matrix of training images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 785)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the labels. Note that the system we're writing identifies the digit 4, so the labels that are originally 4 become 1, and the others become 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(filename):\n",
    "    # Open and unzip the file of images:\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        # Skip the header bytes:\n",
    "        f.read(8)\n",
    "        # Read all the labels into a list:\n",
    "        all_labels = f.read()\n",
    "        # Reshape the list of labels into a one-column matrix:\n",
    "        labels_matrix = np.frombuffer(all_labels, dtype=np.uint8).reshape(-1, 1)\n",
    "        # Encode the matrix so that all 4s become 1, and other digits become 0s:\n",
    "        return (labels_matrix == 4).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60K labels, each with value 1 if the digit is a five, and 0 otherwise\n",
    "Y_train = load_labels(\"./mnist/train-labels-idx1-ubyte.gz\")\n",
    "\n",
    "# 10000 labels, with the same encoding as Y_train\n",
    "Y_test = load_labels(\"./mnist/t10k-labels-idx1-ubyte.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training labels should be a matrix with 1 column and 60K rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here is the code of the binary classifier notebook. Nothing changed in any of these functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w):\n",
    "    return sigmoid(np.matmul(X, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, Y, w):\n",
    "    predictions = predict(X, w)\n",
    "    first_term = Y * np.log(predictions)\n",
    "    second_term = (1 - Y) * np.log(1 - predictions)\n",
    "    return -np.average(first_term + second_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, Y, w):\n",
    "    return np.matmul(X.T, (predict(X, w) - Y)) / X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, iterations, lr):\n",
    "    w = np.zeros((X.shape[1], 1))\n",
    "    for i in range(iterations):\n",
    "        print(\"Iteration %4d => Loss: %.20f\" % (i, loss(X, Y, w)))\n",
    "        w -= gradient(X, Y, w) * lr\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run training with 200 iterations and a pretty small learning rate. This is going to take a minute or two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 => Loss: 0.69314718055994528623\n",
      "Iteration    1 => Loss: 0.80017553516096073807\n",
      "Iteration    2 => Loss: 0.55723828104553563279\n",
      "Iteration    3 => Loss: 0.32973175390660297568\n",
      "Iteration    4 => Loss: 0.18334562770359463801\n",
      "Iteration    5 => Loss: 0.16726190795891721086\n",
      "Iteration    6 => Loss: 0.15898112692349325448\n",
      "Iteration    7 => Loss: 0.15231619727522435759\n",
      "Iteration    8 => Loss: 0.14690314715421115555\n",
      "Iteration    9 => Loss: 0.14226046885782328566\n",
      "Iteration   10 => Loss: 0.13819528582968026997\n",
      "Iteration   11 => Loss: 0.13458801318399327140\n",
      "Iteration   12 => Loss: 0.13135763336291331194\n",
      "Iteration   13 => Loss: 0.12844347920976570410\n",
      "Iteration   14 => Loss: 0.12579789856333775666\n",
      "Iteration   15 => Loss: 0.12338265705502209080\n",
      "Iteration   16 => Loss: 0.12116664919212588591\n",
      "Iteration   17 => Loss: 0.11912428830004077873\n",
      "Iteration   18 => Loss: 0.11723432263009328502\n",
      "Iteration   19 => Loss: 0.11547894406452016702\n",
      "Iteration   20 => Loss: 0.11384310912932420201\n",
      "Iteration   21 => Loss: 0.11231401507466454159\n",
      "Iteration   22 => Loss: 0.11088069134776687430\n",
      "Iteration   23 => Loss: 0.10953367780312914248\n",
      "Iteration   24 => Loss: 0.10826476887860417286\n",
      "Iteration   25 => Loss: 0.10706680845000472735\n",
      "Iteration   26 => Loss: 0.10593352399664567043\n",
      "Iteration   27 => Loss: 0.10485939153399403967\n",
      "Iteration   28 => Loss: 0.10383952482912724158\n",
      "Iteration   29 => Loss: 0.10286958393237527476\n",
      "Iteration   30 => Loss: 0.10194569918810016118\n",
      "Iteration   31 => Loss: 0.10106440773614490392\n",
      "Iteration   32 => Loss: 0.10022260015857921422\n",
      "Iteration   33 => Loss: 0.09941747541778596375\n",
      "Iteration   34 => Loss: 0.09864650261038587775\n",
      "Iteration   35 => Loss: 0.09790738835511735016\n",
      "Iteration   36 => Loss: 0.09719804886218728901\n",
      "Iteration   37 => Loss: 0.09651658591202759430\n",
      "Iteration   38 => Loss: 0.09586126611418502308\n",
      "Iteration   39 => Loss: 0.09523050293076536221\n",
      "Iteration   40 => Loss: 0.09462284103989404560\n",
      "Iteration   41 => Loss: 0.09403694268795621813\n",
      "Iteration   42 => Loss: 0.09347157573870197578\n",
      "Iteration   43 => Loss: 0.09292560317555312310\n",
      "Iteration   44 => Loss: 0.09239797385287715215\n",
      "Iteration   45 => Loss: 0.09188771432436401709\n",
      "Iteration   46 => Loss: 0.09139392160332822368\n",
      "Iteration   47 => Loss: 0.09091575673185540996\n",
      "Iteration   48 => Loss: 0.09045243905407952811\n",
      "Iteration   49 => Loss: 0.09000324110420331036\n",
      "Iteration   50 => Loss: 0.08956748403271110048\n",
      "Iteration   51 => Loss: 0.08914453350501283513\n",
      "Iteration   52 => Loss: 0.08873379601585684728\n",
      "Iteration   53 => Loss: 0.08833471557054910217\n",
      "Iteration   54 => Loss: 0.08794677069055288621\n",
      "Iteration   55 => Loss: 0.08756947170660721314\n",
      "Iteration   56 => Loss: 0.08720235830725463211\n",
      "Iteration   57 => Loss: 0.08684499731474035200\n",
      "Iteration   58 => Loss: 0.08649698066373742722\n",
      "Iteration   59 => Loss: 0.08615792356136441066\n",
      "Iteration   60 => Loss: 0.08582746280955858009\n",
      "Iteration   61 => Loss: 0.08550525527311639284\n",
      "Iteration   62 => Loss: 0.08519097647866370748\n",
      "Iteration   63 => Loss: 0.08488431933151355246\n",
      "Iteration   64 => Loss: 0.08458499293884771952\n",
      "Iteration   65 => Loss: 0.08429272152894942693\n",
      "Iteration   66 => Loss: 0.08400724345734390808\n",
      "Iteration   67 => Loss: 0.08372831029169434947\n",
      "Iteration   68 => Loss: 0.08345568596817302953\n",
      "Iteration   69 => Loss: 0.08318914601279212007\n",
      "Iteration   70 => Loss: 0.08292847682185712499\n",
      "Iteration   71 => Loss: 0.08267347499630288421\n",
      "Iteration   72 => Loss: 0.08242394672520100829\n",
      "Iteration   73 => Loss: 0.08217970721419830260\n",
      "Iteration   74 => Loss: 0.08194058015506182346\n",
      "Iteration   75 => Loss: 0.08170639723287798084\n",
      "Iteration   76 => Loss: 0.08147699766778325470\n",
      "Iteration   77 => Loss: 0.08125222778840031357\n",
      "Iteration   78 => Loss: 0.08103194063441658546\n",
      "Iteration   79 => Loss: 0.08081599558597893362\n",
      "Iteration   80 => Loss: 0.08060425801779005917\n",
      "Iteration   81 => Loss: 0.08039659897598279426\n",
      "Iteration   82 => Loss: 0.08019289487601860555\n",
      "Iteration   83 => Loss: 0.07999302722001234989\n",
      "Iteration   84 => Loss: 0.07979688233202207603\n",
      "Iteration   85 => Loss: 0.07960435110996975927\n",
      "Iteration   86 => Loss: 0.07941532879297127923\n",
      "Iteration   87 => Loss: 0.07922971474295639749\n",
      "Iteration   88 => Loss: 0.07904741223955269458\n",
      "Iteration   89 => Loss: 0.07886832828729119227\n",
      "Iteration   90 => Loss: 0.07869237343426843667\n",
      "Iteration   91 => Loss: 0.07851946160146838771\n",
      "Iteration   92 => Loss: 0.07834950992201161757\n",
      "Iteration   93 => Loss: 0.07818243858965627513\n",
      "Iteration   94 => Loss: 0.07801817071592799524\n",
      "Iteration   95 => Loss: 0.07785663219530397638\n",
      "Iteration   96 => Loss: 0.07769775157791977693\n",
      "Iteration   97 => Loss: 0.07754145994930815300\n",
      "Iteration   98 => Loss: 0.07738769081671499628\n",
      "Iteration   99 => Loss: 0.07723638000157201366\n",
      "Iteration  100 => Loss: 0.07708746553773564159\n",
      "Iteration  101 => Loss: 0.07694088757513034560\n",
      "Iteration  102 => Loss: 0.07679658828846096241\n",
      "Iteration  103 => Loss: 0.07665451179068138998\n",
      "Iteration  104 => Loss: 0.07651460405092998240\n",
      "Iteration  105 => Loss: 0.07637681281666172661\n",
      "Iteration  106 => Loss: 0.07624108753972605457\n",
      "Iteration  107 => Loss: 0.07610737930615615876\n",
      "Iteration  108 => Loss: 0.07597564076945195755\n",
      "Iteration  109 => Loss: 0.07584582608715291496\n",
      "Iteration  110 => Loss: 0.07571789086051104734\n",
      "Iteration  111 => Loss: 0.07559179207708677239\n",
      "Iteration  112 => Loss: 0.07546748805610159483\n",
      "Iteration  113 => Loss: 0.07534493839639280766\n",
      "Iteration  114 => Loss: 0.07522410392682525593\n",
      "Iteration  115 => Loss: 0.07510494665902410494\n",
      "Iteration  116 => Loss: 0.07498742974230165892\n",
      "Iteration  117 => Loss: 0.07487151742065882576\n",
      "Iteration  118 => Loss: 0.07475717499174959479\n",
      "Iteration  119 => Loss: 0.07464436876770345897\n",
      "Iteration  120 => Loss: 0.07453306603770719352\n",
      "Iteration  121 => Loss: 0.07442323503225389825\n",
      "Iteration  122 => Loss: 0.07431484488897174834\n",
      "Iteration  123 => Loss: 0.07420786561995118547\n",
      "Iteration  124 => Loss: 0.07410226808049333314\n",
      "Iteration  125 => Loss: 0.07399802393920722199\n",
      "Iteration  126 => Loss: 0.07389510564938805981\n",
      "Iteration  127 => Loss: 0.07379348642161161209\n",
      "Iteration  128 => Loss: 0.07369314019748474109\n",
      "Iteration  129 => Loss: 0.07359404162449480202\n",
      "Iteration  130 => Loss: 0.07349616603190384234\n",
      "Iteration  131 => Loss: 0.07339948940763703356\n",
      "Iteration  132 => Loss: 0.07330398837611724894\n",
      "Iteration  133 => Loss: 0.07320964017700022641\n",
      "Iteration  134 => Loss: 0.07311642264476768405\n",
      "Iteration  135 => Loss: 0.07302431418913787886\n",
      "Iteration  136 => Loss: 0.07293329377625486221\n",
      "Iteration  137 => Loss: 0.07284334091062086292\n",
      "Iteration  138 => Loss: 0.07275443561773647927\n",
      "Iteration  139 => Loss: 0.07266655842741732985\n",
      "Iteration  140 => Loss: 0.07257969035775510558\n",
      "Iteration  141 => Loss: 0.07249381289969497599\n",
      "Iteration  142 => Loss: 0.07240890800220063650\n",
      "Iteration  143 => Loss: 0.07232495805798144772\n",
      "Iteration  144 => Loss: 0.07224194588975646480\n",
      "Iteration  145 => Loss: 0.07215985473703165343\n",
      "Iteration  146 => Loss: 0.07207866824336790768\n",
      "Iteration  147 => Loss: 0.07199837044411858122\n",
      "Iteration  148 => Loss: 0.07191894575461608985\n",
      "Iteration  149 => Loss: 0.07184037895878850333\n",
      "Iteration  150 => Loss: 0.07176265519818768313\n",
      "Iteration  151 => Loss: 0.07168575996141150763\n",
      "Iteration  152 => Loss: 0.07160967907390375364\n",
      "Iteration  153 => Loss: 0.07153439868811570235\n",
      "Iteration  154 => Loss: 0.07145990527401442638\n",
      "Iteration  155 => Loss: 0.07138618560992360251\n",
      "Iteration  156 => Loss: 0.07131322677368306939\n",
      "Iteration  157 => Loss: 0.07124101613411416845\n",
      "Iteration  158 => Loss: 0.07116954134277847510\n",
      "Iteration  159 => Loss: 0.07109879032601811033\n",
      "Iteration  160 => Loss: 0.07102875127726653026\n",
      "Iteration  161 => Loss: 0.07095941264961853900\n",
      "Iteration  162 => Loss: 0.07089076314864994899\n",
      "Iteration  163 => Loss: 0.07082279172547650825\n",
      "Iteration  164 => Loss: 0.07075548757004317124\n",
      "Iteration  165 => Loss: 0.07068884010463429013\n",
      "Iteration  166 => Loss: 0.07062283897759680240\n",
      "Iteration  167 => Loss: 0.07055747405726771337\n",
      "Iteration  168 => Loss: 0.07049273542609832410\n",
      "Iteration  169 => Loss: 0.07042861337496766905\n",
      "Iteration  170 => Loss: 0.07036509839767815522\n",
      "Iteration  171 => Loss: 0.07030218118562632512\n",
      "Iteration  172 => Loss: 0.07023985262264249851\n",
      "Iteration  173 => Loss: 0.07017810377999274263\n",
      "Iteration  174 => Loss: 0.07011692591153746712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  175 => Loss: 0.07005631044904066240\n",
      "Iteration  176 => Loss: 0.06999624899762441066\n",
      "Iteration  177 => Loss: 0.06993673333136346537\n",
      "Iteration  178 => Loss: 0.06987775538901459804\n",
      "Iteration  179 => Loss: 0.06981930726987627123\n",
      "Iteration  180 => Loss: 0.06976138122977343370\n",
      "Iteration  181 => Loss: 0.06970396967716367687\n",
      "Iteration  182 => Loss: 0.06964706516935990910\n",
      "Iteration  183 => Loss: 0.06959066040886570381\n",
      "Iteration  184 => Loss: 0.06953474823981939390\n",
      "Iteration  185 => Loss: 0.06947932164454294346\n",
      "Iteration  186 => Loss: 0.06942437374019248819\n",
      "Iteration  187 => Loss: 0.06936989777550613134\n",
      "Iteration  188 => Loss: 0.06931588712764681637\n",
      "Iteration  189 => Loss: 0.06926233529913597420\n",
      "Iteration  190 => Loss: 0.06920923591487564142\n",
      "Iteration  191 => Loss: 0.06915658271925567702\n",
      "Iteration  192 => Loss: 0.06910436957334328834\n",
      "Iteration  193 => Loss: 0.06905259045215231262\n",
      "Iteration  194 => Loss: 0.06900123944198929826\n",
      "Iteration  195 => Loss: 0.06895031073787410980\n",
      "Iteration  196 => Loss: 0.06889979864103247531\n",
      "Iteration  197 => Loss: 0.06884969755645821421\n",
      "Iteration  198 => Loss: 0.06880000199054282783\n",
      "Iteration  199 => Loss: 0.06875070654877019072\n"
     ]
    }
   ],
   "source": [
    "w = train(X_train, Y_train, iterations=200, lr=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a matrix of 785 weights–one for each pixel in the images, plus one for the bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(785, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the first ten predictions, and compare them with the first ten labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(predict(X_test, w))[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
