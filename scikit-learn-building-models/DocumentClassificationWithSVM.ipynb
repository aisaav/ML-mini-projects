{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SVM Model for Text Classification\n",
    "\n",
    "This notebook will cover how we can use Support Vector Machines for text classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the 20 New Groups data set from the scikit-learn library\n",
    "* Data comprises a number of emails, articles and other text documents\n",
    "* Each document falls into one of 20 categories of \"News\"\n",
    "* Use a classification model to predict the news category given the document text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the first document in our data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.data[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View all the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The target is represented by numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 4, 4, ..., 3, 1, 8])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a bag of words from our document list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the word counts for the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 56979)\t3\n",
      "  (0, 75358)\t2\n",
      "  (0, 123162)\t2\n",
      "  (0, 118280)\t2\n",
      "  (0, 50527)\t2\n",
      "  (0, 124031)\t2\n",
      "  (0, 85354)\t1\n",
      "  (0, 114688)\t1\n",
      "  (0, 111322)\t1\n",
      "  (0, 123984)\t1\n",
      "  (0, 37780)\t5\n",
      "  (0, 68532)\t3\n",
      "  (0, 114731)\t5\n",
      "  (0, 87620)\t1\n",
      "  (0, 95162)\t1\n",
      "  (0, 64095)\t1\n",
      "  (0, 98949)\t1\n",
      "  (0, 90379)\t1\n",
      "  (0, 118983)\t1\n",
      "  (0, 89362)\t3\n",
      "  (0, 79666)\t1\n",
      "  (0, 40998)\t1\n",
      "  (0, 92081)\t1\n",
      "  (0, 76032)\t1\n",
      "  (0, 4605)\t1\n",
      "  :\t:\n",
      "  (0, 37565)\t1\n",
      "  (0, 113986)\t1\n",
      "  (0, 83256)\t1\n",
      "  (0, 86001)\t1\n",
      "  (0, 51730)\t1\n",
      "  (0, 109271)\t1\n",
      "  (0, 128026)\t1\n",
      "  (0, 96144)\t1\n",
      "  (0, 78784)\t1\n",
      "  (0, 63363)\t1\n",
      "  (0, 90252)\t1\n",
      "  (0, 123989)\t1\n",
      "  (0, 67156)\t1\n",
      "  (0, 128402)\t2\n",
      "  (0, 62221)\t1\n",
      "  (0, 57308)\t1\n",
      "  (0, 76722)\t1\n",
      "  (0, 94362)\t1\n",
      "  (0, 78955)\t1\n",
      "  (0, 114428)\t1\n",
      "  (0, 66098)\t1\n",
      "  (0, 35187)\t1\n",
      "  (0, 35983)\t1\n",
      "  (0, 128420)\t1\n",
      "  (0, 86580)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_train_counts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get TF-IDF Weights using TfidfTransformer\n",
    "This is different from TfidfVectorizer:\n",
    "* TfidfVectorizer takes in a list of documents as input and produces a TF-IDF weighted bag of words\n",
    "* TfidfTransformer takes in a regular bag of words and creates a TF-IDF weighted bag of words\n",
    "* TfidfVectorizer == CountVectorizer + TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Viewing the TF-IDF weights for first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 128420)\t0.04278499079283093\n",
      "  (0, 128402)\t0.05922294083277842\n",
      "  (0, 128026)\t0.060622095889758885\n",
      "  (0, 124931)\t0.08882569909852546\n",
      "  (0, 124031)\t0.10798795154169122\n",
      "  (0, 123989)\t0.08207027465330353\n",
      "  (0, 123984)\t0.036854292634593756\n",
      "  (0, 123796)\t0.049437556160455476\n",
      "  (0, 123292)\t0.14534718515938805\n",
      "  (0, 123162)\t0.2597090245735688\n",
      "  (0, 118983)\t0.037085978050619146\n",
      "  (0, 118280)\t0.2118680720828169\n",
      "  (0, 115475)\t0.042472629883573\n",
      "  (0, 114731)\t0.14447275512784058\n",
      "  (0, 114688)\t0.06214070986309586\n",
      "  (0, 114579)\t0.03671830826216751\n",
      "  (0, 114455)\t0.12287762616208957\n",
      "  (0, 114428)\t0.05511105154696676\n",
      "  (0, 113986)\t0.17691750674853082\n",
      "  (0, 111322)\t0.01915671802495043\n",
      "  (0, 109581)\t0.10809248404447917\n",
      "  (0, 109271)\t0.10844724822064673\n",
      "  (0, 108252)\t0.07526015712540636\n",
      "  (0, 106116)\t0.09869734624201922\n",
      "  (0, 104813)\t0.08462829788929047\n",
      "  :\t:\n",
      "  (0, 56979)\t0.057470154074851294\n",
      "  (0, 51793)\t0.13412921037839678\n",
      "  (0, 51730)\t0.09714744057976722\n",
      "  (0, 50527)\t0.054614286588587246\n",
      "  (0, 50111)\t0.08452530453354308\n",
      "  (0, 48620)\t0.11603642565244157\n",
      "  (0, 48618)\t0.10015015488700592\n",
      "  (0, 45295)\t0.06621689096551565\n",
      "  (0, 42876)\t0.04951998622750595\n",
      "  (0, 40998)\t0.0780136819691811\n",
      "  (0, 37780)\t0.38133891259493113\n",
      "  (0, 37565)\t0.03431760442478462\n",
      "  (0, 37433)\t0.06908779999621749\n",
      "  (0, 35983)\t0.03770448563619875\n",
      "  (0, 35612)\t0.1328075333935896\n",
      "  (0, 35187)\t0.09353930598317124\n",
      "  (0, 34995)\t0.16713176431412632\n",
      "  (0, 34181)\t0.08716420445779295\n",
      "  (0, 32311)\t0.029911858621703466\n",
      "  (0, 28615)\t0.10278592160069082\n",
      "  (0, 27436)\t0.037098931990947055\n",
      "  (0, 26073)\t0.09534869974107982\n",
      "  (0, 18299)\t0.138749083899155\n",
      "  (0, 16574)\t0.14155752531572685\n",
      "  (0, 4605)\t0.06332603952480323\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Linear Support Vector Classifier\n",
    "* penalty specifies whether to use L1 norm or L2 norm\n",
    "    * Like with Lasso and Ridge, choose whether to minimize sum of absolute values or sum of squares of coefficients\n",
    "* dual specifies whether to solve the primal or dual optimization problem\n",
    "    * A primal optimization problem (e.g. increase revenue) can have an equivalent dual problem (e.g. reduce costs) (this is a gross oversimplification - a lot of math needed to explain in detail)\n",
    "    * In our example, the primal optimization could be to maximize distance between our model and nearest points on either side of it. This will have a corresponding dual optimization problem\n",
    "    * scikit-learn recommends that dual=False when there are more samples than features (which is the case in this example)\n",
    "* tol represents a tolerance for the algorithm to consider when trying to maximize or minimize an ojective function\n",
    "    * if the model is within the tolerance of the maximum or minimum, it is not refined further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf_svc = LinearSVC(penalty=\"l2\", dual=False, tol=1e-3)\n",
    "clf_svc.fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternatively, a scikit-learn Pipeline can be used\n",
    "* Pipeline is a sequence of transformations with an estimator specified in the final step\n",
    "* The output of one transformation is passed as input to the next transformation\n",
    "* The pipeline returns a model of the type specified in the estimator\n",
    "* When the fit() method of the model is called with arguments, the arguments are passed through the transformation steps before actually being applied to the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf_svc_pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf',LinearSVC(penalty=\"l2\", dual=False, tol=0.001))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example:\n",
    "* we pass the document corpus and the labels to the pipeline classifier\n",
    "* The CountVectorizer takes the corpus and creates a bag of words\n",
    "* The TfidfTransformer takes the bag of words and produces a TF-IDF weighted bag\n",
    "* The LinearSVC model applies the fit method with the TF-IDF weighted bag and the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 LinearSVC(C=1.0, class_weight=None, dual=False,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='squared_hinge', max_iter=1000,\n",
       "                           multi_class='ovr', penalty='l2', random_state=None,\n",
       "                           tol=0.001, verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svc_pipeline.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain the test data which we will use to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make the predictions using our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf_svc_pipeline.predict(twenty_test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the accuracy of the model\n",
    "Remember, there are 20 categories, so wild guesses will result in an accuracy of about 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_svm = accuracy_score(twenty_test.target, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8534253850238981"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How good is our model if we just used the word counts without transforming to TF-IDF weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svc_pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf',LinearSVC(penalty=\"l2\", dual=False, tol=0.001))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7983271375464684"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svc_pipeline.fit(twenty_train.data, twenty_train.target)\n",
    "predicted = clf_svc_pipeline.predict(twenty_test.data)\n",
    "\n",
    "acc_svm = accuracy_score(twenty_test.target, predicted)\n",
    "acc_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
